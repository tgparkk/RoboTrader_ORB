#!/usr/bin/env python3
"""
ML ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦

ë°ì´í„°: ml_dataset.csv
ëª¨ë¸: LightGBM (XGBoostì˜ ë¹ ë¥¸ ëŒ€ì•ˆ)
ëª©í‘œ: íŒ¨í„´ ì‹ í˜¸ì˜ ìŠ¹/íŒ¨ ì˜ˆì¸¡
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
    precision_recall_curve
)
import lightgbm as lgb
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œë„ ì‘ë™
import matplotlib.pyplot as plt
import pickle

sys.stdout.reconfigure(encoding='utf-8')


def load_and_prepare_data(csv_file: str = 'ml_dataset.csv'):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    print("=" * 70)
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    print("=" * 70)

    df = pd.read_csv(csv_file, encoding='utf-8-sig')
    print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")

    # ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œê±°
    meta_cols = ['stock_code', 'pattern_id', 'timestamp', 'sell_reason', 'profit_rate']
    feature_cols = [col for col in df.columns if col not in meta_cols + ['label']]

    X = df[feature_cols].copy()
    y = df['label'].copy()

    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (signal_type)
    le = LabelEncoder()
    if 'signal_type' in X.columns:
        X['signal_type'] = le.fit_transform(X['signal_type'])

    print(f"\níŠ¹ì§•(feature) ìˆ˜: {len(feature_cols)}")
    print(f"ë¼ë²¨ ë¶„í¬: ìŠ¹ë¦¬={y.sum()} ({y.mean()*100:.1f}%), íŒ¨ë°°={len(y)-y.sum()} ({(1-y.mean())*100:.1f}%)")

    return X, y, feature_cols, le


def train_lightgbm_model(X_train, y_train, X_val, y_val, feature_names):
    """LightGBM ëª¨ë¸ í•™ìŠµ"""
    print("\n" + "=" * 70)
    print("ğŸš€ LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 70)

    # LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê³¼ì í•© ë°©ì§€)
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,  # íŠ¸ë¦¬ ë³µì¡ë„ (ì‘ì„ìˆ˜ë¡ ë‹¨ìˆœ)
        'learning_rate': 0.05,
        'feature_fraction': 0.8,  # ê° íŠ¸ë¦¬ì—ì„œ ì‚¬ìš©í•  íŠ¹ì§• ë¹„ìœ¨
        'bagging_fraction': 0.8,  # ê° íŠ¸ë¦¬ì—ì„œ ì‚¬ìš©í•  ë°ì´í„° ë¹„ìœ¨
        'bagging_freq': 5,
        'min_data_in_leaf': 20,  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (ê³¼ì í•© ë°©ì§€)
        'max_depth': 6,  # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ (ê³¼ì í•© ë°©ì§€)
        'verbose': -1,
        'seed': 42
    }

    # ë°ì´í„°ì…‹ ìƒì„±
    lgb_train = lgb.Dataset(X_train, y_train, feature_name=feature_names)
    lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train, feature_name=feature_names)

    # í•™ìŠµ
    print("í•™ìŠµ ì¤‘...")
    model = lgb.train(
        params,
        lgb_train,
        num_boost_round=500,  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
        valid_sets=[lgb_train, lgb_val],
        valid_names=['train', 'valid'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),  # 50íšŒ ê°œì„  ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
            lgb.log_evaluation(period=50)
        ]
    )

    print(f"âœ… í•™ìŠµ ì™„ë£Œ: ìµœì  ë°˜ë³µ íšŸìˆ˜ = {model.best_iteration}")

    return model


def evaluate_model(model, X_test, y_test, feature_names):
    """ëª¨ë¸ í‰ê°€"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ëª¨ë¸ í‰ê°€")
    print("=" * 70)

    # ì˜ˆì¸¡
    y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)
    y_pred = (y_pred_proba >= 0.5).astype(int)

    # AUC ìŠ¤ì½”ì–´
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"\nğŸ¯ AUC Score: {auc:.4f}")

    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_test, y_pred)
    print("\ní˜¼ë™ í–‰ë ¬:")
    print("              ì˜ˆì¸¡ íŒ¨ë°°  ì˜ˆì¸¡ ìŠ¹ë¦¬")
    print(f"ì‹¤ì œ íŒ¨ë°°        {cm[0,0]:4d}      {cm[0,1]:4d}")
    print(f"ì‹¤ì œ ìŠ¹ë¦¬        {cm[1,0]:4d}      {cm[1,1]:4d}")

    # ë¶„ë¥˜ ë¦¬í¬íŠ¸
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_pred, target_names=['íŒ¨ë°°', 'ìŠ¹ë¦¬'], digits=4))

    # ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥
    print("\nğŸšï¸ ì„ê³„ê°’ë³„ ì„±ëŠ¥:")
    print("-" * 60)
    print("ì„ê³„ê°’ | ì •ë°€ë„ | ì¬í˜„ìœ¨ | ìŠ¹ë¥ ì˜ˆì¸¡ | ì‹¤ì œìŠ¹ë¥  | ê±°ë˜ìˆ˜")
    print("-" * 60)

    for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
        y_pred_thresh = (y_pred_proba >= threshold).astype(int)

        # ìŠ¹ë¦¬ë¡œ ì˜ˆì¸¡í•œ ìƒ˜í”Œë“¤
        predicted_wins = y_pred_thresh == 1

        if predicted_wins.sum() > 0:
            precision = y_test[predicted_wins].mean()  # ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ìŠ¹ë¦¬ ë¹„ìœ¨
            recall = y_test[predicted_wins].sum() / y_test.sum()  # ì „ì²´ ìŠ¹ë¦¬ ì¤‘ ì¡ì•„ë‚¸ ë¹„ìœ¨
            predicted_win_rate = predicted_wins.mean() * 100
            actual_win_rate = precision * 100
            n_trades = predicted_wins.sum()
        else:
            precision = 0
            recall = 0
            predicted_win_rate = 0
            actual_win_rate = 0
            n_trades = 0

        print(f" {threshold:.1f}    | {precision:6.1%} | {recall:6.1%} | "
              f"{predicted_win_rate:7.1f}% | {actual_win_rate:7.1f}% | {n_trades:5d}ê±´")

    return y_pred_proba, auc


def plot_feature_importance(model, feature_names, top_n=20):
    """íŠ¹ì§• ì¤‘ìš”ë„ ì‹œê°í™”"""
    print("\n" + "=" * 70)
    print("ğŸ” íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„")
    print("=" * 70)

    importance = model.feature_importance(importance_type='gain')
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)

    print(f"\nìƒìœ„ {top_n}ê°œ ì¤‘ìš” íŠ¹ì§•:")
    for i, row in feature_importance_df.head(top_n).iterrows():
        print(f"  {row['feature']:35s}: {row['importance']:8.0f}")

    # ê·¸ë˜í”„ ì €ì¥
    plt.figure(figsize=(10, 8))
    top_features = feature_importance_df.head(top_n)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importance (Gain)')
    plt.title(f'Top {top_n} Feature Importance')
    plt.tight_layout()
    plt.savefig('ml_feature_importance.png', dpi=150)
    print("\nâœ… íŠ¹ì§• ì¤‘ìš”ë„ ê·¸ë˜í”„ ì €ì¥: ml_feature_importance.png")

    return feature_importance_df


def plot_roc_curve(y_test, y_pred_proba):
    """ROC ê³¡ì„  ì‹œê°í™”"""
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'LightGBM (AUC = {auc:.4f})')
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('ml_roc_curve.png', dpi=150)
    print("âœ… ROC ê³¡ì„  ì €ì¥: ml_roc_curve.png")


def save_model(model, feature_names, label_encoder, filename='ml_model.pkl'):
    """ëª¨ë¸ ì €ì¥"""
    model_data = {
        'model': model,
        'feature_names': feature_names,
        'label_encoder': label_encoder
    }

    with open(filename, 'wb') as f:
        pickle.dump(model_data, f)

    print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filename}")
    print(f"   íŒŒì¼ í¬ê¸°: {Path(filename).stat().st_size / 1024:.1f} KB")


def cross_validate_model(X, y, feature_names, n_splits=5):
    """êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì•ˆì •ì„± í‰ê°€"""
    print("\n" + "=" * 70)
    print(f"ğŸ”„ {n_splits}-Fold êµì°¨ ê²€ì¦")
    print("=" * 70)

    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_data_in_leaf': 20,
        'max_depth': 6,
        'verbose': -1,
        'seed': 42
    }

    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    cv_scores = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]

        lgb_train = lgb.Dataset(X_train_cv, y_train_cv, feature_name=feature_names)
        lgb_val = lgb.Dataset(X_val_cv, y_val_cv, reference=lgb_train, feature_name=feature_names)

        model_cv = lgb.train(
            params,
            lgb_train,
            num_boost_round=500,
            valid_sets=[lgb_val],
            valid_names=['valid'],
            callbacks=[
                lgb.early_stopping(stopping_rounds=50, verbose=False),
                lgb.log_evaluation(period=0)  # ë¡œê·¸ ë¹„í™œì„±í™”
            ]
        )

        y_pred_proba = model_cv.predict(X_val_cv, num_iteration=model_cv.best_iteration)
        auc_score = roc_auc_score(y_val_cv, y_pred_proba)
        cv_scores.append(auc_score)

        print(f"Fold {fold}: AUC = {auc_score:.4f}")

    print(f"\ní‰ê·  AUC: {np.mean(cv_scores):.4f} (Â± {np.std(cv_scores):.4f})")
    print(f"ìµœì†Œ AUC: {np.min(cv_scores):.4f}")
    print(f"ìµœëŒ€ AUC: {np.max(cv_scores):.4f}")

    return cv_scores


def main():
    print("\n" + "=" * 70)
    print("ğŸ¤– íŠ¸ë ˆì´ë”© íŒ¨í„´ ML ëª¨ë¸ í•™ìŠµ")
    print("=" * 70)

    # 1. ë°ì´í„° ë¡œë“œ
    X, y, feature_names, label_encoder = load_and_prepare_data()

    # 2. ì‹œê°„ ê¸°ë°˜ ë¶„í•  (ì²˜ìŒ 60%ë¡œ í•™ìŠµ, ë‚˜ì¤‘ 40%ë¡œ ê²€ì¦/í…ŒìŠ¤íŠ¸)
    # (ëœë¤ ë¶„í• ì€ ë¯¸ë˜ ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜)
    train_split = int(len(X) * 0.6)
    val_split = int(len(X) * 0.8)

    X_train = X.iloc[:train_split]
    X_val = X.iloc[train_split:val_split]
    X_test = X.iloc[val_split:]

    y_train = y.iloc[:train_split]
    y_val = y.iloc[train_split:val_split]
    y_test = y.iloc[val_split:]

    print(f"\në°ì´í„° ë¶„í• :")
    print(f"  í•™ìŠµ ì„¸íŠ¸: {len(X_train):4d}ê°œ (ìŠ¹ë¥  {y_train.mean()*100:.1f}%)")
    print(f"  ê²€ì¦ ì„¸íŠ¸: {len(X_val):4d}ê°œ (ìŠ¹ë¥  {y_val.mean()*100:.1f}%)")
    print(f"  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(X_test):4d}ê°œ (ìŠ¹ë¥  {y_test.mean()*100:.1f}%)")

    # 3. êµì°¨ ê²€ì¦ (í•™ìŠµ ì„¸íŠ¸ë§Œ ì‚¬ìš©)
    cv_scores = cross_validate_model(X_train, y_train, feature_names, n_splits=5)

    # 4. ëª¨ë¸ í•™ìŠµ
    model = train_lightgbm_model(X_train, y_train, X_val, y_val, feature_names)

    # 5. í‰ê°€
    y_pred_proba, auc = evaluate_model(model, X_test, y_test, feature_names)

    # 6. íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
    feature_importance_df = plot_feature_importance(model, feature_names, top_n=20)

    # 7. ROC ê³¡ì„ 
    plot_roc_curve(y_test, y_pred_proba)

    # 8. ëª¨ë¸ ì €ì¥
    save_model(model, feature_names, label_encoder, filename='ml_model.pkl')

    print("\n" + "=" * 70)
    print("âœ… ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ!")
    print("=" * 70)
    print("\nìƒì„±ëœ íŒŒì¼:")
    print("  - ml_model.pkl (í•™ìŠµëœ ëª¨ë¸)")
    print("  - ml_feature_importance.png (íŠ¹ì§• ì¤‘ìš”ë„)")
    print("  - ml_roc_curve.png (ROC ê³¡ì„ )")

    # ìµœì¢… ê¶Œì¥ì‚¬í•­
    print("\n" + "=" * 70)
    print("ğŸ’¡ ì‚¬ìš© ê¶Œì¥ì‚¬í•­")
    print("=" * 70)
    print(f"1. ëª¨ë¸ AUC: {auc:.4f}")

    if auc >= 0.60:
        print("   â†’ ëª¨ë¸ì´ ìœ ì˜ë¯¸í•œ ì˜ˆì¸¡ë ¥ì„ ë³´ì…ë‹ˆë‹¤.")
        print("   â†’ ì„ê³„ê°’ 0.6 ì´ìƒ ì‹ í˜¸ë§Œ ê±°ë˜í•˜ë©´ ìŠ¹ë¥  í–¥ìƒ ê°€ëŠ¥")
    elif auc >= 0.55:
        print("   â†’ ëª¨ë¸ì´ ì•½ê°„ì˜ ì˜ˆì¸¡ë ¥ì„ ë³´ì…ë‹ˆë‹¤.")
        print("   â†’ ê¸°ì¡´ í•„í„°ì™€ ë³‘í–‰ ì‚¬ìš© ê¶Œì¥")
    else:
        print("   â†’ ëª¨ë¸ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ì¶”ê°€ íŠ¹ì§• í•„ìš”")

    print("\n2. ë‹¤ìŒ ë‹¨ê³„:")
    print("   - batch_signal_replay_ml.py ë¡œ ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ìˆ˜í–‰")
    print("   - ì„ê³„ê°’ ì¡°ì •í•˜ì—¬ ìµœì  ìŠ¹ë¥ /ê±°ë˜ìˆ˜ ë°¸ëŸ°ìŠ¤ ì°¾ê¸°")


if __name__ == '__main__':
    try:
        import lightgbm
    except ImportError:
        print("âŒ lightgbmì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("  pip install lightgbm scikit-learn matplotlib")
        sys.exit(1)

    main()
