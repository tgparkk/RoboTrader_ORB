#!/usr/bin/env python3
"""
ML ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ (V2 - ì¼ë´‰ ë°ì´í„° í¬í•¨)

ë°ì´í„°: ml_dataset_v2.csv
ëª¨ë¸: LightGBM
ëª©í‘œ: íŒ¨í„´ ì‹ í˜¸ì˜ ìŠ¹/íŒ¨ ì˜ˆì¸¡ (ì¼ë´‰ ë°ì´í„° ë° ê¸°ìˆ ì  ì§€í‘œ í¬í•¨)
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
    precision_recall_curve
)
import lightgbm as lgb
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pickle

sys.stdout.reconfigure(encoding='utf-8')


def load_and_prepare_data(csv_file: str = 'ml_dataset_v2.csv'):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    print("=" * 70)
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘... (V2 - ì¼ë´‰ ë°ì´í„° í¬í•¨)")
    print("=" * 70)

    df = pd.read_csv(csv_file, encoding='utf-8-sig', low_memory=False)
    print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")

    # ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œê±°
    meta_cols = ['stock_code', 'pattern_id', 'timestamp', 'sell_reason', 'profit_rate']
    feature_cols = [col for col in df.columns if col not in meta_cols + ['label']]

    X = df[feature_cols].copy()
    y = df['label'].copy()

    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (signal_type)
    le = LabelEncoder()
    if 'signal_type' in X.columns:
        X['signal_type'] = le.fit_transform(X['signal_type'].astype(str))

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    X = X.fillna(0)

    print(f"\níŠ¹ì§•(feature) ìˆ˜: {len(feature_cols)}")
    print(f"ë¼ë²¨ ë¶„í¬: ìŠ¹ë¦¬={y.sum()} ({y.mean()*100:.1f}%), íŒ¨ë°°={len(y)-y.sum()} ({(1-y.mean())*100:.1f}%)")

    # íŠ¹ì„± ê·¸ë£¹ë³„ ê°œìˆ˜
    pattern_features = [col for col in feature_cols if not col.startswith(('daily_', 'price_change_', 'volume_change_', 'rsi_', 'macd_', 'bb_', 'ma', 'volatility_', 'high_low_'))]
    daily_features = [col for col in feature_cols if col not in pattern_features]

    print(f"\níŠ¹ì„± êµ¬ì„±:")
    print(f"  - íŒ¨í„´ íŠ¹ì„±: {len(pattern_features)}ê°œ")
    print(f"  - ì¼ë´‰/ê¸°ìˆ ì§€í‘œ íŠ¹ì„±: {len(daily_features)}ê°œ")

    return X, y, feature_cols, le


def train_lightgbm_model(X_train, y_train, X_val, y_val, feature_names):
    """LightGBM ëª¨ë¸ í•™ìŠµ"""
    print("\n" + "=" * 70)
    print("ğŸš€ LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘ (V2)")
    print("=" * 70)

    # LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê³¼ì í•© ë°©ì§€ ê°•í™”)
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.03,  # í•™ìŠµë¥  ë‚®ì¶¤ (ë” ì•ˆì •ì )
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_data_in_leaf': 20,
        'max_depth': 6,
        'lambda_l1': 0.5,  # L1 ì •ê·œí™”
        'lambda_l2': 0.5,  # L2 ì •ê·œí™”
        'verbose': -1,
        'seed': 42
    }

    # ë°ì´í„°ì…‹ ìƒì„±
    lgb_train = lgb.Dataset(X_train, y_train, feature_name=feature_names)
    lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train, feature_name=feature_names)

    # í•™ìŠµ
    print("í•™ìŠµ ì¤‘...")
    model = lgb.train(
        params,
        lgb_train,
        num_boost_round=1000,
        valid_sets=[lgb_train, lgb_val],
        valid_names=['train', 'valid'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=50)
        ]
    )

    print(f"âœ… í•™ìŠµ ì™„ë£Œ: ìµœì  ë°˜ë³µ íšŸìˆ˜ = {model.best_iteration}")
    return model


def evaluate_model(model, X_test, y_test, feature_names, output_dir='ml_results_v2'):
    """ëª¨ë¸ í‰ê°€ ë° ê²°ê³¼ ì €ì¥"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ëª¨ë¸ í‰ê°€")
    print("=" * 70)

    Path(output_dir).mkdir(exist_ok=True)

    # ì˜ˆì¸¡
    y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)
    y_pred = (y_pred_proba > 0.5).astype(int)

    # ì„±ëŠ¥ ì§€í‘œ
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_pred, target_names=['íŒ¨ë°°', 'ìŠ¹ë¦¬']))

    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_test, y_pred)
    print("\ní˜¼ë™ í–‰ë ¬:")
    print(f"             ì˜ˆì¸¡: íŒ¨ë°°  ì˜ˆì¸¡: ìŠ¹ë¦¬")
    print(f"ì‹¤ì œ íŒ¨ë°°:   {cm[0][0]:>6}    {cm[0][1]:>6}")
    print(f"ì‹¤ì œ ìŠ¹ë¦¬:   {cm[1][0]:>6}    {cm[1][1]:>6}")

    # AUC
    auc_score = roc_auc_score(y_test, y_pred_proba)
    print(f"\nğŸ¯ Test AUC: {auc_score:.4f}")

    # ROC Curve ì €ì¥
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
    plt.figure(figsize=(10, 6))
    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.4f})')
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - V2 Model')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'{output_dir}/roc_curve_v2.png', dpi=150)
    print(f"âœ… ROC Curve ì €ì¥: {output_dir}/roc_curve_v2.png")

    # Feature Importance
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False)

    print("\nğŸ” Top 20 ì¤‘ìš” íŠ¹ì„±:")
    for i, row in importance_df.head(20).iterrows():
        print(f"  {row['feature']:30s}: {row['importance']:>10.0f}")

    # Feature Importance ì €ì¥
    plt.figure(figsize=(12, 10))
    top_features = importance_df.head(30)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importance (Gain)')
    plt.title('Top 30 Feature Importance - V2 Model')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig(f'{output_dir}/feature_importance_v2.png', dpi=150)
    print(f"âœ… Feature Importance ì €ì¥: {output_dir}/feature_importance_v2.png")

    # ì¼ë´‰ íŠ¹ì„± vs íŒ¨í„´ íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ
    pattern_features = [f for f in feature_names if not f.startswith(('daily_', 'price_change_', 'volume_change_', 'rsi_', 'macd_', 'bb_', 'ma', 'volatility_', 'high_low_'))]
    daily_features = [f for f in feature_names if f not in pattern_features]

    pattern_importance = importance_df[importance_df['feature'].isin(pattern_features)]['importance'].sum()
    daily_importance = importance_df[importance_df['feature'].isin(daily_features)]['importance'].sum()
    total_importance = pattern_importance + daily_importance

    print(f"\nğŸ“Š íŠ¹ì„± ê·¸ë£¹ë³„ ì¤‘ìš”ë„:")
    print(f"  íŒ¨í„´ íŠ¹ì„±: {pattern_importance:>10.0f} ({pattern_importance/total_importance*100:.1f}%)")
    print(f"  ì¼ë´‰/ê¸°ìˆ ì§€í‘œ: {daily_importance:>10.0f} ({daily_importance/total_importance*100:.1f}%)")

    # CSV ì €ì¥
    importance_df.to_csv(f'{output_dir}/feature_importance_v2.csv', index=False, encoding='utf-8-sig')

    return auc_score, importance_df


def cross_validate_model(X, y, feature_names):
    """êµì°¨ ê²€ì¦"""
    print("\n" + "=" * 70)
    print("ğŸ”„ 5-Fold êµì°¨ ê²€ì¦")
    print("=" * 70)

    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.03,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_data_in_leaf': 20,
        'max_depth': 6,
        'lambda_l1': 0.5,
        'lambda_l2': 0.5,
        'verbose': -1,
        'seed': 42
    }

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = []

    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        lgb_train = lgb.Dataset(X_train_fold, y_train_fold, feature_name=feature_names)
        lgb_val = lgb.Dataset(X_val_fold, y_val_fold, reference=lgb_train, feature_name=feature_names)

        model_cv = lgb.train(
            params,
            lgb_train,
            num_boost_round=1000,
            valid_sets=[lgb_val],
            callbacks=[
                lgb.early_stopping(stopping_rounds=50),
                lgb.log_evaluation(period=0)  # ì¶œë ¥ ì–µì œ
            ]
        )

        y_pred_proba = model_cv.predict(X_val_fold, num_iteration=model_cv.best_iteration)
        auc = roc_auc_score(y_val_fold, y_pred_proba)
        cv_scores.append(auc)
        print(f"Fold {fold}: AUC = {auc:.4f}")

    print(f"\ní‰ê·  CV AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
    return cv_scores


def save_model(model, label_encoder, feature_names, output_file='ml_model_v2.pkl'):
    """ëª¨ë¸ ì €ì¥"""
    model_data = {
        'model': model,
        'label_encoder': label_encoder,
        'feature_names': feature_names,
        'version': 'v2',
        'description': 'LightGBM model with daily data and technical indicators'
    }

    with open(output_file, 'wb') as f:
        pickle.dump(model_data, f)

    print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"   ëª¨ë¸ í¬ê¸°: {Path(output_file).stat().st_size / 1024:.1f} KB")


def main():
    print("=" * 70)
    print("ğŸ¤– ML ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ V2 (ì¼ë´‰ ë°ì´í„° í¬í•¨)")
    print("=" * 70)

    # 1. ë°ì´í„° ë¡œë“œ
    X, y, feature_names, label_encoder = load_and_prepare_data('ml_dataset_v2.csv')

    # 2. ë°ì´í„° ë¶„í•  (ì‹œê°„ ê¸°ë°˜: 60% í•™ìŠµ, 20% ê²€ì¦, 20% í…ŒìŠ¤íŠ¸)
    print("\n" + "=" * 70)
    print("âœ‚ï¸ ë°ì´í„° ë¶„í•  (ì‹œê°„ ê¸°ë°˜)")
    print("=" * 70)

    train_split = int(len(X) * 0.6)
    val_split = int(len(X) * 0.8)

    X_train = X.iloc[:train_split]
    y_train = y.iloc[:train_split]

    X_val = X.iloc[train_split:val_split]
    y_val = y.iloc[train_split:val_split]

    X_test = X.iloc[val_split:]
    y_test = y.iloc[val_split:]

    print(f"í•™ìŠµ ì„¸íŠ¸: {len(X_train)}ê°œ (ìŠ¹ë¥  {y_train.mean()*100:.1f}%)")
    print(f"ê²€ì¦ ì„¸íŠ¸: {len(X_val)}ê°œ (ìŠ¹ë¥  {y_val.mean()*100:.1f}%)")
    print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(X_test)}ê°œ (ìŠ¹ë¥  {y_test.mean()*100:.1f}%)")

    # 3. êµì°¨ ê²€ì¦
    cv_scores = cross_validate_model(X, y, feature_names)

    # 4. ëª¨ë¸ í•™ìŠµ
    model = train_lightgbm_model(X_train, y_train, X_val, y_val, feature_names)

    # 5. ëª¨ë¸ í‰ê°€
    test_auc, importance_df = evaluate_model(model, X_test, y_test, feature_names)

    # 6. ëª¨ë¸ ì €ì¥
    save_model(model, label_encoder, feature_names, 'ml_model_v2.pkl')

    # 7. ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“ˆ ìµœì¢… ì„±ê³¼ ìš”ì•½")
    print("=" * 70)
    print(f"êµì°¨ ê²€ì¦ AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
    print(f"í…ŒìŠ¤íŠ¸ AUC:     {test_auc:.4f}")
    print(f"\níŠ¹ì„± ìˆ˜: {len(feature_names)}ê°œ")
    print(f"ìƒ˜í”Œ ìˆ˜: {len(X)}ê°œ (í•™ìŠµ {len(X_train)}, ê²€ì¦ {len(X_val)}, í…ŒìŠ¤íŠ¸ {len(X_test)})")

    # V1 ëª¨ë¸ê³¼ ë¹„êµ (ìˆëŠ” ê²½ìš°)
    if Path('ml_model.pkl').exists():
        print("\n" + "=" * 70)
        print("ğŸ”„ V1 ëª¨ë¸ê³¼ ë¹„êµ")
        print("=" * 70)
        print("V1 ëª¨ë¸ (íŒ¨í„´ë§Œ): íŠ¹ì„± 26ê°œ")
        print("V2 ëª¨ë¸ (íŒ¨í„´+ì¼ë´‰): íŠ¹ì„± 68ê°œ")
        print(f"íŠ¹ì„± ì¦ê°€: +{len(feature_names) - 26}ê°œ (+{(len(feature_names) - 26) / 26 * 100:.0f}%)")


if __name__ == '__main__':
    main()
