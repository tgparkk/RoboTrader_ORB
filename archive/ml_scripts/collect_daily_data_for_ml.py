#!/usr/bin/env python3
"""
ML í•™ìŠµìš© ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘

pattern_data_logì— ìˆëŠ” ì¢…ëª©ë“¤ì˜ ì¼ë´‰ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ cache/dailyì— ì €ì¥í•©ë‹ˆë‹¤.
ì´í›„ ml_prepare_dataset_v2.py ì‹¤í–‰ ì‹œ ìºì‹œì—ì„œ ì¼ë´‰ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import sys
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Set, Dict
import pickle

sys.path.append(str(Path(__file__).parent))

from api.kis_api_manager import KISAPIManager
from api.kis_market_api import get_inquire_daily_itemchartprice
from utils.logger import setup_logger

logger = setup_logger(__name__)


def extract_stock_codes_from_patterns() -> Dict[str, Set[str]]:
    """
    pattern_data_logì—ì„œ ì¢…ëª©ì½”ë“œì™€ ë‚ ì§œ ì¶”ì¶œ

    Returns:
        Dict[stock_code, Set[dates]]: ì¢…ëª©ì½”ë“œë³„ ê±°ë˜ ë‚ ì§œë“¤
    """
    pattern_log_dir = Path('pattern_data_log')

    if not pattern_log_dir.exists():
        logger.error(f"íŒ¨í„´ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {pattern_log_dir}")
        return {}

    stock_dates = {}  # {stock_code: {date1, date2, ...}}

    jsonl_files = sorted(pattern_log_dir.glob('pattern_data_*.jsonl'))
    logger.info(f"íŒ¨í„´ ë¡œê·¸ íŒŒì¼ {len(jsonl_files)}ê°œ ìŠ¤ìº” ì¤‘...")

    for jsonl_file in jsonl_files:
        try:
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue

                    pattern = json.loads(line)

                    # ë§¤ë§¤ ê²°ê³¼ê°€ ìˆëŠ” ê²ƒë§Œ (í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©ë¨)
                    if not pattern.get('trade_result', {}).get('trade_executed', False):
                        continue

                    stock_code = pattern.get('stock_code', '')
                    timestamp = pattern.get('timestamp', '')

                    if not stock_code or not timestamp:
                        continue

                    try:
                        dt = datetime.fromisoformat(timestamp)
                        trade_date = dt.strftime('%Y%m%d')

                        if stock_code not in stock_dates:
                            stock_dates[stock_code] = set()
                        stock_dates[stock_code].add(trade_date)

                    except:
                        continue

        except Exception as e:
            logger.warning(f"{jsonl_file.name} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    logger.info(f"ì¶”ì¶œ ì™„ë£Œ: {len(stock_dates)}ê°œ ì¢…ëª©, ì´ {sum(len(dates) for dates in stock_dates.values())}ê±´ ê±°ë˜")

    return stock_dates


def collect_daily_data(stock_code: str, trade_date: str, lookback_days: int = 60) -> bool:
    """
    íŠ¹ì • ì¢…ëª©ì˜ ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘

    Args:
        stock_code: ì¢…ëª©ì½”ë“œ
        trade_date: ê±°ë˜ì¼ (YYYYMMDD)
        lookback_days: ê³¼ê±° Nì¼ ë°ì´í„°

    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    # ìºì‹œ ë””ë ‰í† ë¦¬
    cache_dir = Path('cache/daily_data')
    cache_dir.mkdir(parents=True, exist_ok=True)

    # ìºì‹œ íŒŒì¼ëª…
    cache_file = cache_dir / f"{stock_code}_{trade_date}_d{lookback_days}.pkl"

    # ì´ë¯¸ ìºì‹œ ì¡´ì¬
    if cache_file.exists():
        return True

    # API í˜¸ì¶œ
    try:
        end_date = datetime.strptime(trade_date, '%Y%m%d')
        start_date = end_date - timedelta(days=lookback_days + 30)  # ì—¬ìœ ìˆê²Œ

        df = get_inquire_daily_itemchartprice(
            output_dv="2",
            div_code="J",
            itm_no=stock_code,
            inqr_strt_dt=start_date.strftime('%Y%m%d'),
            inqr_end_dt=trade_date,
            period_code="D",
            adj_prc="1"
        )

        if df is None or len(df) == 0:
            logger.warning(f"{stock_code} {trade_date}: ì¼ë´‰ ë°ì´í„° ì—†ìŒ")
            return False

        # ë°ì´í„° íƒ€ì… ë³€í™˜
        df = df.copy()
        df['stck_bsop_date'] = df['stck_bsop_date'].astype(str)
        df['stck_oprc'] = pd.to_numeric(df['stck_oprc'], errors='coerce').fillna(0)
        df['stck_hgpr'] = pd.to_numeric(df['stck_hgpr'], errors='coerce').fillna(0)
        df['stck_lwpr'] = pd.to_numeric(df['stck_lwpr'], errors='coerce').fillna(0)
        df['stck_clpr'] = pd.to_numeric(df['stck_clpr'], errors='coerce').fillna(0)
        df['acml_vol'] = pd.to_numeric(df['acml_vol'], errors='coerce').fillna(0)

        # ì •ë ¬ ë° í•„í„°ë§
        df = df.sort_values('stck_bsop_date').reset_index(drop=True)
        df = df[df['stck_bsop_date'] <= trade_date]
        df = df.tail(lookback_days)

        # ìºì‹œ ì €ì¥
        with open(cache_file, 'wb') as f:
            pickle.dump(df, f)

        logger.debug(f"âœ“ {stock_code} {trade_date}: {len(df)}ì¼ ì €ì¥")
        return True

    except Exception as e:
        logger.warning(f"{stock_code} {trade_date} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return False


def main():
    import pandas as pd
    import sys
    sys.stdout.reconfigure(encoding='utf-8')

    print("=" * 70)
    print("ML í•™ìŠµìš© ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘")
    print("=" * 70)

    # 1. API ì´ˆê¸°í™”
    print("\nğŸ“¡ API ì´ˆê¸°í™” ì¤‘...")
    api_manager = KISAPIManager()
    if not api_manager.initialize():
        print("âŒ API ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    print("âœ… API ì´ˆê¸°í™” ì™„ë£Œ")

    # 2. íŒ¨í„´ ë¡œê·¸ì—ì„œ ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
    print("\nğŸ“‚ íŒ¨í„´ ë¡œê·¸ ë¶„ì„ ì¤‘...")
    stock_dates = extract_stock_codes_from_patterns()

    if not stock_dates:
        print("âŒ ìˆ˜ì§‘í•  ë°ì´í„° ì—†ìŒ")
        return

    # ì¢…ëª©ë³„ ê±°ë˜ íšŸìˆ˜ ê³„ì‚°
    stock_counts = [(code, len(dates)) for code, dates in stock_dates.items()]
    stock_counts.sort(key=lambda x: x[1], reverse=True)

    print(f"\nğŸ“Š ìˆ˜ì§‘ ëŒ€ìƒ:")
    print(f"   ì´ ì¢…ëª©: {len(stock_dates)}ê°œ")
    print(f"   ì´ ê±°ë˜: {sum(count for _, count in stock_counts)}ê±´")
    print(f"\n   ìƒìœ„ 10ê°œ ì¢…ëª©:")
    for code, count in stock_counts[:10]:
        print(f"      {code}: {count}ê±´")

    # 3. ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘
    print(f"\nğŸ”„ ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")

    total = 0
    success = 0
    failed = 0
    cached = 0

    # ìºì‹œ í™•ì¸
    cache_dir = Path('cache/daily_data')
    existing_caches = set()
    if cache_dir.exists():
        for cache_file in cache_dir.glob('*.pkl'):
            # íŒŒì¼ëª…: {stock_code}_{date}_d{lookback}.pkl
            parts = cache_file.stem.split('_')
            if len(parts) >= 3:
                code = parts[0]
                date = parts[1]
                existing_caches.add((code, date))

    for stock_code, dates in stock_dates.items():
        for trade_date in sorted(dates):
            total += 1

            # ì´ë¯¸ ìºì‹œ ì¡´ì¬
            if (stock_code, trade_date) in existing_caches:
                cached += 1
                continue

            # ìˆ˜ì§‘
            if collect_daily_data(stock_code, trade_date, lookback_days=60):
                success += 1
            else:
                failed += 1

            # ì§„í–‰ìƒí™© ì¶œë ¥
            if total % 10 == 0:
                print(f"   ì§„í–‰: {total}ê±´ (ì„±ê³µ {success}, ì‹¤íŒ¨ {failed}, ìºì‹œ {cached})")

    # 4. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“ˆ ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 70)
    print(f"ì´ ì²˜ë¦¬: {total}ê±´")
    print(f"  - ìºì‹œ ì‚¬ìš©: {cached}ê±´")
    print(f"  - ì‹ ê·œ ìˆ˜ì§‘ ì„±ê³µ: {success}ê±´")
    print(f"  - ìˆ˜ì§‘ ì‹¤íŒ¨: {failed}ê±´")

    success_rate = (cached + success) / total * 100 if total > 0 else 0
    print(f"\nì„±ê³µë¥ : {success_rate:.1f}%")

    if success > 0 or cached > 0:
        print("\nâœ… ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ML ë°ì´í„°ì…‹ì„ ì¬ìƒì„±í•˜ì„¸ìš”:")
        print("   python ml_prepare_dataset_v2.py")


if __name__ == '__main__':
    main()
