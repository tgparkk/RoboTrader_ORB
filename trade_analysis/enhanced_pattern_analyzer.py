"""
í–¥ìƒëœ íŒ¨í„´ ë¶„ì„ê¸°
70ê°œ ì´ìƒì˜ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
from pathlib import Path
import pickle
import json
import re
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

from utils.logger import setup_logger
from utils.korean_time import now_kst
from enhanced_feature_extractor import EnhancedFeatureExtractor

@dataclass
class EnhancedPatternFeature:
    """í–¥ìƒëœ íŒ¨í„´ íŠ¹ì„± ë°ì´í„° í´ë˜ìŠ¤"""
    feature_name: str
    value: float
    weight: float = 1.0
    importance: float = 0.0
    p_value: float = 1.0
    description: str = ""

@dataclass
class EnhancedWinLossPattern:
    """í–¥ìƒëœ ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë°ì´í„° í´ë˜ìŠ¤"""
    stock_code: str
    signal_date: str
    signal_time: str
    is_win: bool
    return_pct: float
    enhanced_features: Dict[str, float]
    combined_score: float = 0.0
    prediction_confidence: float = 0.0

class EnhancedPatternAnalyzer:
    """í–¥ìƒëœ íŒ¨í„´ ë¶„ì„ê¸°"""
    
    def __init__(self, logger=None):
        self.logger = logger or setup_logger(__name__)
        self.patterns: List[EnhancedWinLossPattern] = []
        self.feature_extractor = EnhancedFeatureExtractor()
        self.feature_importance: Dict[str, float] = {}
        self.model = None
        self.scaler = StandardScaler()
        self.win_threshold: float = 0.6
        
    def load_trade_logs(self, log_dir: str = "signal_replay_log") -> List[Dict]:
        """ê±°ë˜ ë¡œê·¸ ë¡œë“œ"""
        try:
            log_path = Path(log_dir)
            if not log_path.exists():
                self.logger.warning(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {log_dir}")
                return []
            
            all_logs = []
            log_files = list(log_path.glob("*.txt"))
            
            self.logger.info(f"ğŸ“ {len(log_files)}ê°œ ë¡œê·¸ íŒŒì¼ ë°œê²¬")
            
            for log_file in log_files:
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    logs = self._parse_log_content(content, log_file.name)
                    all_logs.extend(logs)
                    
                except Exception as e:
                    self.logger.error(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {log_file.name}: {e}")
                    continue
            
            self.logger.info(f"âœ… ì´ {len(all_logs)}ê°œì˜ ê±°ë˜ ë¡œê·¸ ë¡œë“œ ì™„ë£Œ")
            return all_logs
            
        except Exception as e:
            self.logger.error(f"ê±°ë˜ ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_log_content(self, content: str, filename: str) -> List[Dict]:
        """ë¡œê·¸ ë‚´ìš© íŒŒì‹±"""
        logs = []
        lines = content.split('\n')
        
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        date_match = re.search(r'(\d{8})', filename)
        signal_date = date_match.group(1) if date_match else now_kst().strftime("%Y%m%d")
        
        current_stock = None
        
        for line in lines:
            line = line.strip()
            
            # ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
            stock_match = re.search(r'=== (\d{6}) -', line)
            if stock_match:
                current_stock = stock_match.group(1)
                continue
            
            # ë§¤ìˆ˜/ë§¤ë„ ì •ë³´ ì¶”ì¶œ
            trade_match = re.search(r'(\d{2}:\d{2}) ë§¤ìˆ˜\[.*?\] @([\d,]+) â†’ (\d{2}:\d{2}) ë§¤ë„\[.*?\] @([\d,]+) \(([+-]?\d+\.\d+)%\)', line)
            if trade_match and current_stock:
                buy_time = trade_match.group(1)
                buy_price = float(trade_match.group(2).replace(',', ''))
                sell_time = trade_match.group(3)
                sell_price = float(trade_match.group(4).replace(',', ''))
                return_pct = float(trade_match.group(5))
                
                is_win = return_pct > 0
                
                log_data = {
                    'stock_code': current_stock,
                    'signal_date': signal_date,
                    'signal_time': buy_time,
                    'is_win': is_win,
                    'return_pct': return_pct,
                    'buy_price': buy_price,
                    'sell_price': sell_price,
                    'raw_line': line
                }
                
                logs.append(log_data)
        
        return logs
    
    def analyze_patterns(self, log_dir: str = "signal_replay_log") -> Dict[str, Any]:
        """í–¥ìƒëœ íŒ¨í„´ ë¶„ì„"""
        try:
            self.logger.info("ğŸ” í–¥ìƒëœ ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë¶„ì„ ì‹œì‘...")
            
            # 1. ê±°ë˜ ë¡œê·¸ ë¡œë“œ
            trade_logs = self.load_trade_logs(log_dir)
            if not trade_logs:
                self.logger.warning("ë¶„ì„í•  ê±°ë˜ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {}
            
            # 2. í–¥ìƒëœ íŠ¹ì„± ì¶”ì¶œ
            patterns = []
            for log in trade_logs:
                try:
                    # ì¼ë´‰ ë°ì´í„° ë¡œë“œ
                    daily_data = self._load_daily_data(log['stock_code'])
                    if daily_data is None or daily_data.empty:
                        continue
                    
                    # í–¥ìƒëœ íŠ¹ì„± ì¶”ì¶œ
                    enhanced_features = self.feature_extractor.extract_all_features(
                        daily_data, log['signal_date']
                    )
                    
                    if not enhanced_features:
                        continue
                    
                    # íŒ¨í„´ ìƒì„±
                    pattern = EnhancedWinLossPattern(
                        stock_code=log['stock_code'],
                        signal_date=log['signal_date'],
                        signal_time=log['signal_time'],
                        is_win=log['is_win'],
                        return_pct=log['return_pct'],
                        enhanced_features=enhanced_features
                    )
                    
                    patterns.append(pattern)
                    
                except Exception as e:
                    self.logger.debug(f"íŒ¨í„´ ìƒì„± ì‹¤íŒ¨ {log['stock_code']}: {e}")
                    continue
            
            self.patterns = patterns
            self.logger.info(f"âœ… {len(patterns)}ê°œ í–¥ìƒëœ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
            
            # 3. íŠ¹ì„±ë³„ ë¶„ì„
            analysis_result = self._analyze_enhanced_features(patterns)
            
            # 4. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
            ml_result = self._train_ml_model(patterns)
            
            # 5. ê²°ê³¼ í†µí•©
            final_result = {
                **analysis_result,
                'ml_model': ml_result,
                'total_patterns': len(patterns),
                'win_patterns': len([p for p in patterns if p.is_win]),
                'loss_patterns': len([p for p in patterns if not p.is_win]),
                'win_rate': len([p for p in patterns if p.is_win]) / len(patterns) if patterns else 0
            }
            
            # 6. ê²°ê³¼ ì €ì¥
            self._save_analysis_result(final_result)
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒëœ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_daily_data(self, stock_code: str) -> Optional[pd.DataFrame]:
        """ì¼ë´‰ ë°ì´í„° ë¡œë“œ"""
        try:
            daily_cache_dir = Path("cache/daily_data")
            daily_file = daily_cache_dir / f"{stock_code}_daily.pkl"
            
            if not daily_file.exists():
                return None
                
            with open(daily_file, 'rb') as f:
                data = pickle.load(f)
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬ ë° ë°ì´í„° íƒ€ì… ë³€í™˜
            if 'stck_bsop_date' in data.columns:
                data['date'] = pd.to_datetime(data['stck_bsop_date'])
            if 'stck_clpr' in data.columns:
                data['close'] = pd.to_numeric(data['stck_clpr'], errors='coerce')
            if 'stck_oprc' in data.columns:
                data['open'] = pd.to_numeric(data['stck_oprc'], errors='coerce')
            if 'stck_hgpr' in data.columns:
                data['high'] = pd.to_numeric(data['stck_hgpr'], errors='coerce')
            if 'stck_lwpr' in data.columns:
                data['low'] = pd.to_numeric(data['stck_lwpr'], errors='coerce')
            if 'acml_vol' in data.columns:
                data['volume'] = pd.to_numeric(data['acml_vol'], errors='coerce')
                
            return data.sort_values('date').reset_index(drop=True)
            
        except Exception as e:
            self.logger.debug(f"ì¼ë´‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {stock_code}: {e}")
            return None
    
    def _analyze_enhanced_features(self, patterns: List[EnhancedWinLossPattern]) -> Dict[str, Any]:
        """í–¥ìƒëœ íŠ¹ì„± ë¶„ì„"""
        try:
            if not patterns:
                return {}
            
            # ìŠ¹ë¦¬/íŒ¨ë°° ê·¸ë£¹ ë¶„ë¦¬
            win_patterns = [p for p in patterns if p.is_win]
            loss_patterns = [p for p in patterns if not p.is_win]
            
            self.logger.info(f"ìŠ¹ë¦¬ íŒ¨í„´: {len(win_patterns)}ê°œ, íŒ¨ë°° íŒ¨í„´: {len(loss_patterns)}ê°œ")
            
            # ëª¨ë“  íŠ¹ì„± ìˆ˜ì§‘
            all_features = set()
            for pattern in patterns:
                all_features.update(pattern.enhanced_features.keys())
            
            # íŠ¹ì„±ë³„ ë¶„ì„
            feature_analysis = {}
            for feature in all_features:
                win_values = [p.enhanced_features.get(feature, 0) for p in win_patterns]
                loss_values = [p.enhanced_features.get(feature, 0) for p in loss_patterns]
                
                if not win_values or not loss_values:
                    continue
                
                win_mean = np.mean(win_values)
                loss_mean = np.mean(loss_values)
                win_std = np.std(win_values)
                loss_std = np.std(loss_values)
                
                # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
                t_stat, p_value = self._enhanced_t_test(win_values, loss_values)
                
                # íŠ¹ì„± ê°€ì¤‘ì¹˜ ê³„ì‚°
                weight = abs(win_mean - loss_mean) / (win_std + loss_std + 1e-8)
                
                feature_analysis[feature] = {
                    'win_mean': win_mean,
                    'loss_mean': loss_mean,
                    'win_std': win_std,
                    'loss_std': loss_std,
                    'difference': win_mean - loss_mean,
                    'weight': weight,
                    't_stat': t_stat,
                    'p_value': p_value,
                    'significance': p_value < 0.05,
                    'effect_size': abs(win_mean - loss_mean) / np.sqrt((win_std**2 + loss_std**2) / 2)
                }
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            max_weight = max([fa['weight'] for fa in feature_analysis.values()]) if feature_analysis else 1
            for feature in feature_analysis:
                feature_analysis[feature]['normalized_weight'] = feature_analysis[feature]['weight'] / max_weight
            
            # ìƒìœ„ íŠ¹ì„± ì„ íƒ
            top_features = self._get_top_enhanced_features(feature_analysis)
            
            return {
                'feature_analysis': feature_analysis,
                'top_features': top_features,
                'total_features': len(feature_analysis),
                'significant_features': len([f for f in feature_analysis.values() if f['significance']])
            }
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒëœ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _enhanced_t_test(self, group1: List[float], group2: List[float]) -> Tuple[float, float]:
        """í–¥ìƒëœ t-test"""
        try:
            n1, n2 = len(group1), len(group2)
            if n1 < 2 or n2 < 2:
                return 0.0, 1.0
            
            mean1, mean2 = np.mean(group1), np.mean(group2)
            var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
            
            # Welch's t-test (ì´ë¶„ì‚° ê°€ì •)
            se = np.sqrt(var1/n1 + var2/n2)
            t_stat = (mean1 - mean2) / se
            
            # ììœ ë„ ê³„ì‚°
            df = (var1/n1 + var2/n2)**2 / ((var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1))
            
            # p-value ê³„ì‚° (ê°„ë‹¨í•œ ê·¼ì‚¬)
            p_value = 2 * (1 - self._t_cdf(abs(t_stat), df))
            
            return t_stat, p_value
            
        except Exception as e:
            self.logger.debug(f"t-test ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0, 1.0
    
    def _t_cdf(self, t: float, df: float) -> float:
        """t-ë¶„í¬ ëˆ„ì ë¶„í¬í•¨ìˆ˜ ê·¼ì‚¬"""
        # ê°„ë‹¨í•œ ê·¼ì‚¬ (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ êµ¬í˜„ í•„ìš”)
        if df > 30:
            return 0.5 + 0.5 * np.tanh(t / 2)
        else:
            return 0.5 + 0.5 * np.tanh(t / (1 + df/10))
    
    def _get_top_enhanced_features(self, feature_analysis: Dict[str, Dict]) -> List[Dict]:
        """ìƒìœ„ í–¥ìƒëœ íŠ¹ì„± ì„ íƒ"""
        try:
            # ê°€ì¤‘ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_features = sorted(
                feature_analysis.items(),
                key=lambda x: x[1]['normalized_weight'],
                reverse=True
            )
            
            top_features = []
            for feature, analysis in sorted_features[:20]:  # ìƒìœ„ 20ê°œ
                top_features.append({
                    'feature': feature,
                    'analysis': analysis,
                    'rank': len(top_features) + 1
                })
            
            return top_features
            
        except Exception as e:
            self.logger.error(f"ìƒìœ„ íŠ¹ì„± ì„ íƒ ì‹¤íŒ¨: {e}")
            return []
    
    def _train_ml_model(self, patterns: List[EnhancedWinLossPattern]) -> Dict[str, Any]:
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ"""
        try:
            if len(patterns) < 50:
                self.logger.warning("ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return {}
            
            # íŠ¹ì„± ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            feature_matrix = []
            labels = []
            
            for pattern in patterns:
                features = []
                for feature_name in sorted(pattern.enhanced_features.keys()):
                    features.append(pattern.enhanced_features[feature_name])
                feature_matrix.append(features)
                labels.append(1 if pattern.is_win else 0)
            
            X = np.array(feature_matrix)
            y = np.array(labels)
            
            # íŠ¹ì„± ì •ê·œí™”
            X_scaled = self.scaler.fit_transform(X)
            
            # íŠ¹ì„± ì„ íƒ
            selector = SelectKBest(f_classif, k=min(20, X.shape[1]))
            X_selected = selector.fit_transform(X_scaled, y)
            
            # ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(self.model, X_selected, y, cv=5)
            
            # ëª¨ë¸ í•™ìŠµ
            self.model.fit(X_selected, y)
            
            # íŠ¹ì„± ì¤‘ìš”ë„
            feature_names = [name for name in sorted(patterns[0].enhanced_features.keys())]
            selected_features = [feature_names[i] for i in selector.get_support(indices=True)]
            feature_importance = dict(zip(selected_features, self.model.feature_importances_))
            
            # ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€
            y_pred = self.model.predict(X_selected)
            accuracy = np.mean(y_pred == y)
            
            return {
                'model_type': 'RandomForest',
                'cv_scores': cv_scores.tolist(),
                'cv_mean': np.mean(cv_scores),
                'cv_std': np.std(cv_scores),
                'accuracy': accuracy,
                'feature_importance': feature_importance,
                'selected_features': selected_features,
                'n_features': len(selected_features)
            }
            
        except Exception as e:
            self.logger.error(f"ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {}
    
    def _save_analysis_result(self, result: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            # JSON ì €ì¥
            with open('enhanced_pattern_analysis.json', 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2, default=str)
            
            # ëª¨ë¸ ì €ì¥
            if self.model is not None:
                import joblib
                joblib.dump(self.model, 'enhanced_pattern_model.pkl')
                joblib.dump(self.scaler, 'enhanced_pattern_scaler.pkl')
            
            self.logger.info("âœ… í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def generate_enhanced_filter_rules(self, analysis_result: Dict[str, Any]) -> Dict[str, Dict]:
        """í–¥ìƒëœ í•„í„° ê·œì¹™ ìƒì„±"""
        try:
            if 'feature_analysis' not in analysis_result:
                return {}
            
            feature_analysis = analysis_result['feature_analysis']
            filter_rules = {}
            
            # ìƒìœ„ íŠ¹ì„±ì— ëŒ€í•œ í•„í„° ê·œì¹™ ìƒì„±
            for feature, analysis in feature_analysis.items():
                if analysis['significance'] and analysis['normalized_weight'] > 0.3:
                    # ìŠ¹ë¦¬ íŒ¨í„´ì˜ íŠ¹ì„±ê°’ ë²”ìœ„ ê³„ì‚°
                    win_mean = analysis['win_mean']
                    win_std = analysis['win_std']
                    
                    # í•„í„° ê·œì¹™ ìƒì„±
                    if analysis['difference'] > 0:
                        # ìŠ¹ë¦¬ê°€ ë” ë†’ì€ ê°’ì„ ê°€ì§€ëŠ” ê²½ìš°
                        threshold = win_mean - 0.5 * win_std
                        rule = {
                            'condition': f"{feature} >= {threshold:.3f}",
                            'weight': analysis['normalized_weight'],
                            'description': f"{feature}ì´ {threshold:.3f} ì´ìƒì´ì–´ì•¼ í•¨",
                            'threshold': threshold,
                            'operator': '>='
                        }
                    else:
                        # ìŠ¹ë¦¬ê°€ ë” ë‚®ì€ ê°’ì„ ê°€ì§€ëŠ” ê²½ìš°
                        threshold = win_mean + 0.5 * win_std
                        rule = {
                            'condition': f"{feature} <= {threshold:.3f}",
                            'weight': analysis['normalized_weight'],
                            'description': f"{feature}ì´ {threshold:.3f} ì´í•˜ì—¬ì•¼ í•¨",
                            'threshold': threshold,
                            'operator': '<='
                        }
                    
                    filter_rules[feature] = rule
            
            return filter_rules
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒëœ í•„í„° ê·œì¹™ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger = setup_logger(__name__)
    
    # í–¥ìƒëœ íŒ¨í„´ ë¶„ì„ê¸° ì‹¤í–‰
    analyzer = EnhancedPatternAnalyzer(logger)
    
    # íŒ¨í„´ ë¶„ì„ ì‹¤í–‰
    results = analyzer.analyze_patterns()
    
    if results:
        print("\n" + "="*80)
        print("ğŸ“Š í–¥ìƒëœ ì¼ë´‰ ê¸°ë°˜ ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë¶„ì„ ê²°ê³¼")
        print("="*80)
        print(f"ì´ íŒ¨í„´ ìˆ˜: {results['total_patterns']}")
        print(f"ìŠ¹ë¦¬ íŒ¨í„´: {results['win_patterns']}")
        print(f"íŒ¨ë°° íŒ¨í„´: {results['loss_patterns']}")
        print(f"ì „ì²´ ìŠ¹ë¥ : {results['win_rate']:.1%}")
        print(f"ì´ íŠ¹ì„± ìˆ˜: {results['total_features']}")
        print(f"ìœ ì˜í•œ íŠ¹ì„± ìˆ˜: {results['significant_features']}")
        
        print("\nğŸ” ìƒìœ„ íŠ¹ì„±:")
        for i, feature_info in enumerate(results['top_features'][:10], 1):
            feature = feature_info['feature']
            analysis = feature_info['analysis']
            print(f"{i:2d}. {feature}")
            print(f"    ìŠ¹ë¦¬ í‰ê· : {analysis['win_mean']:.3f}")
            print(f"    íŒ¨ë°° í‰ê· : {analysis['loss_mean']:.3f}")
            print(f"    ì°¨ì´: {analysis['difference']:+.3f}")
            print(f"    ê°€ì¤‘ì¹˜: {analysis['normalized_weight']:.3f}")
            print(f"    ìœ ì˜ì„±: {'âœ…' if analysis['significance'] else 'âŒ'}")
            print(f"    íš¨ê³¼í¬ê¸°: {analysis['effect_size']:.3f}")
            print()
        
        # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê²°ê³¼
        if 'ml_model' in results and results['ml_model']:
            ml_result = results['ml_model']
            print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥:")
            print(f"  - êµì°¨ê²€ì¦ í‰ê· : {ml_result['cv_mean']:.3f} Â± {ml_result['cv_std']:.3f}")
            print(f"  - ì •í™•ë„: {ml_result['accuracy']:.3f}")
            print(f"  - ì„ íƒëœ íŠ¹ì„± ìˆ˜: {ml_result['n_features']}")
            print()
        
        # í•„í„° ê·œì¹™ ìƒì„±
        filter_rules = analyzer.generate_enhanced_filter_rules(results)
        print("ğŸ¯ ìƒì„±ëœ í–¥ìƒëœ í•„í„° ê·œì¹™:")
        for feature, rule in filter_rules.items():
            print(f"â€¢ {rule['description']} (ê°€ì¤‘ì¹˜: {rule['weight']:.3f})")
    
    else:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
