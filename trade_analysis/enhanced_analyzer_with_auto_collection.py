"""
ìë™ ë°ì´í„° ìˆ˜ì§‘ì´ í†µí•©ëœ í–¥ìƒëœ ë¶„ì„ê¸°
ì¼ë´‰ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•œ í›„ ë¶„ì„ ìˆ˜í–‰
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
from pathlib import Path
import pickle
import json
import re
import time

from utils.logger import setup_logger
from utils.korean_time import now_kst
from auto_daily_data_collector import AutoDailyDataCollector
from enhanced_feature_extractor import EnhancedFeatureExtractor

@dataclass
class EnhancedPatternFeature:
    """í–¥ìƒëœ íŒ¨í„´ íŠ¹ì„± ë°ì´í„° í´ë˜ìŠ¤"""
    feature_name: str
    value: float
    weight: float = 1.0
    importance: float = 0.0
    p_value: float = 1.0
    description: str = ""

@dataclass
class EnhancedWinLossPattern:
    """í–¥ìƒëœ ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë°ì´í„° í´ë˜ìŠ¤"""
    stock_code: str
    signal_date: str
    signal_time: str
    is_win: bool
    return_pct: float
    enhanced_features: Dict[str, float]
    combined_score: float = 0.0
    prediction_confidence: float = 0.0

class EnhancedAnalyzerWithAutoCollection:
    """ìë™ ë°ì´í„° ìˆ˜ì§‘ì´ í†µí•©ëœ í–¥ìƒëœ ë¶„ì„ê¸°"""
    
    def __init__(self, logger=None):
        self.logger = logger or setup_logger(__name__)
        self.patterns: List[EnhancedWinLossPattern] = []
        self.feature_extractor = EnhancedFeatureExtractor()
        self.data_collector = AutoDailyDataCollector(logger)
        self.feature_importance: Dict[str, float] = {}
        self.win_threshold: float = 0.6
        
    def analyze_with_auto_collection(self, log_dir: str = "signal_replay_log", 
                                   start_date: str = None, end_date: str = None) -> Dict[str, Any]:
        """ìë™ ë°ì´í„° ìˆ˜ì§‘ê³¼ í•¨ê»˜ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰"""
        try:
            self.logger.info("ğŸš€ ìë™ ë°ì´í„° ìˆ˜ì§‘ í†µí•© ë¶„ì„ ì‹œì‘")
            
            # 1. ê±°ë˜ ë¡œê·¸ì—ì„œ ì¢…ëª© ì¶”ì¶œ
            stock_codes = self._extract_stocks_from_logs(log_dir)
            self.logger.info(f"ğŸ“Š ì¶”ì¶œëœ ì¢…ëª©: {len(stock_codes)}ê°œ")
            
            if not stock_codes:
                self.logger.warning("ë¶„ì„í•  ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return {}
            
            # 2. ëˆ„ë½ëœ ì¼ë´‰ ë°ì´í„° ìë™ ìˆ˜ì§‘
            self.logger.info("ğŸ“ˆ ëˆ„ë½ëœ ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            collection_results = self.data_collector.collect_missing_daily_data(
                stock_codes, start_date, end_date
            )
            
            # 3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            quality_report = self.data_collector.verify_data_quality(stock_codes)
            good_quality_stocks = [
                stock for stock, report in quality_report.items() 
                if report['status'] == 'ok' and report['quality_score'] > 0.3
            ]
            
            self.logger.info(f"âœ… ë¶„ì„ ê°€ëŠ¥í•œ ì¢…ëª©: {len(good_quality_stocks)}ê°œ")
            
            # 4. ê±°ë˜ ë¡œê·¸ ë¡œë“œ
            trade_logs = self._load_trade_logs(log_dir)
            self.logger.info(f"ğŸ“‹ ê±°ë˜ ë¡œê·¸: {len(trade_logs)}ê±´")
            
            # 5. í–¥ìƒëœ íŠ¹ì„± ì¶”ì¶œ ë° íŒ¨í„´ ë¶„ì„
            patterns = []
            processed_count = 0
            skipped_count = 0
            
            for log in trade_logs:
                try:
                    # ì¢…ëª©ì´ ë¶„ì„ ê°€ëŠ¥í•œì§€ í™•ì¸
                    if log['stock_code'] not in good_quality_stocks:
                        skipped_count += 1
                        continue
                    
                    # ì¼ë´‰ ë°ì´í„° ë¡œë“œ
                    daily_data = self._load_daily_data(log['stock_code'])
                    if daily_data is None or daily_data.empty:
                        skipped_count += 1
                        continue
                    
                    # í–¥ìƒëœ íŠ¹ì„± ì¶”ì¶œ
                    enhanced_features = self.feature_extractor.extract_all_features(
                        daily_data, log['signal_date']
                    )
                    
                    if not enhanced_features:
                        skipped_count += 1
                        continue
                    
                    # íŒ¨í„´ ìƒì„±
                    pattern = EnhancedWinLossPattern(
                        stock_code=log['stock_code'],
                        signal_date=log['signal_date'],
                        signal_time=log['signal_time'],
                        is_win=log['is_win'],
                        return_pct=log['return_pct'],
                        enhanced_features=enhanced_features
                    )
                    
                    patterns.append(pattern)
                    processed_count += 1
                    
                    if processed_count % 50 == 0:
                        self.logger.info(f"ğŸ“Š ì²˜ë¦¬ ì§„í–‰: {processed_count}ê°œ íŒ¨í„´ ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.debug(f"íŒ¨í„´ ìƒì„± ì‹¤íŒ¨ {log['stock_code']}: {e}")
                    skipped_count += 1
                    continue
            
            self.patterns = patterns
            self.logger.info(f"âœ… {len(patterns)}ê°œ í–¥ìƒëœ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
            self.logger.info(f"â­ï¸ ê±´ë„ˆëœ€: {skipped_count}ê°œ")
            
            # 6. íŠ¹ì„±ë³„ ë¶„ì„
            analysis_result = self._analyze_enhanced_features(patterns)
            
            # 7. ê²°ê³¼ í†µí•©
            final_result = {
                **analysis_result,
                'total_patterns': len(patterns),
                'win_patterns': len([p for p in patterns if p.is_win]),
                'loss_patterns': len([p for p in patterns if not p.is_win]),
                'win_rate': len([p for p in patterns if p.is_win]) / len(patterns) if patterns else 0,
                'processed_stocks': len(good_quality_stocks),
                'skipped_logs': skipped_count,
                'collection_results': collection_results,
                'quality_report': quality_report
            }
            
            # 8. ê²°ê³¼ ì €ì¥
            self._save_analysis_result(final_result)
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"ìë™ ìˆ˜ì§‘ í†µí•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _extract_stocks_from_logs(self, log_dir: str) -> set:
        """ê±°ë˜ ë¡œê·¸ì—ì„œ ì¢…ëª©ì½”ë“œ ì¶”ì¶œ"""
        stock_codes = set()
        log_path = Path(log_dir)
        
        if not log_path.exists():
            self.logger.warning(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {log_dir}")
            return stock_codes
        
        for log_file in log_path.glob("*.txt"):
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ì¢…ëª©ì½”ë“œ ì¶”ì¶œ (=== 6ìë¦¬ìˆ«ì - íŒ¨í„´)
                matches = re.findall(r'=== (\d{6}) -', content)
                stock_codes.update(matches)
                
            except Exception as e:
                self.logger.debug(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {log_file.name}: {e}")
                continue
        
        return stock_codes
    
    def _load_trade_logs(self, log_dir: str) -> List[Dict]:
        """ê±°ë˜ ë¡œê·¸ ë¡œë“œ"""
        try:
            log_path = Path(log_dir)
            if not log_path.exists():
                self.logger.warning(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {log_dir}")
                return []
            
            all_logs = []
            log_files = list(log_path.glob("*.txt"))
            
            self.logger.info(f"ğŸ“ {len(log_files)}ê°œ ë¡œê·¸ íŒŒì¼ ë°œê²¬")
            
            for log_file in log_files:
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    logs = self._parse_log_content(content, log_file.name)
                    all_logs.extend(logs)
                    
                except Exception as e:
                    self.logger.error(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {log_file.name}: {e}")
                    continue
            
            self.logger.info(f"âœ… ì´ {len(all_logs)}ê°œì˜ ê±°ë˜ ë¡œê·¸ ë¡œë“œ ì™„ë£Œ")
            return all_logs
            
        except Exception as e:
            self.logger.error(f"ê±°ë˜ ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_log_content(self, content: str, filename: str) -> List[Dict]:
        """ë¡œê·¸ ë‚´ìš© íŒŒì‹±"""
        logs = []
        lines = content.split('\n')
        
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        date_match = re.search(r'(\d{8})', filename)
        signal_date = date_match.group(1) if date_match else now_kst().strftime("%Y%m%d")
        
        current_stock = None
        
        for line in lines:
            line = line.strip()
            
            # ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
            stock_match = re.search(r'=== (\d{6}) -', line)
            if stock_match:
                current_stock = stock_match.group(1)
                continue
            
            # ë§¤ìˆ˜/ë§¤ë„ ì •ë³´ ì¶”ì¶œ
            trade_match = re.search(r'(\d{2}:\d{2}) ë§¤ìˆ˜\[.*?\] @([\d,]+) â†’ (\d{2}:\d{2}) ë§¤ë„\[.*?\] @([\d,]+) \(([+-]?\d+\.\d+)%\)', line)
            if trade_match and current_stock:
                buy_time = trade_match.group(1)
                buy_price = float(trade_match.group(2).replace(',', ''))
                sell_time = trade_match.group(3)
                sell_price = float(trade_match.group(4).replace(',', ''))
                return_pct = float(trade_match.group(5))
                
                is_win = return_pct > 0
                
                log_data = {
                    'stock_code': current_stock,
                    'signal_date': signal_date,
                    'signal_time': buy_time,
                    'is_win': is_win,
                    'return_pct': return_pct,
                    'buy_price': buy_price,
                    'sell_price': sell_price,
                    'raw_line': line
                }
                
                logs.append(log_data)
        
        return logs
    
    def _load_daily_data(self, stock_code: str) -> Optional[pd.DataFrame]:
        """ì¼ë´‰ ë°ì´í„° ë¡œë“œ"""
        try:
            daily_cache_dir = Path("cache/daily_data")
            daily_file = daily_cache_dir / f"{stock_code}_daily.pkl"
            
            if not daily_file.exists():
                return None
                
            with open(daily_file, 'rb') as f:
                data = pickle.load(f)
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬ ë° ë°ì´í„° íƒ€ì… ë³€í™˜
            if 'stck_bsop_date' in data.columns:
                data['date'] = pd.to_datetime(data['stck_bsop_date'])
            if 'stck_clpr' in data.columns:
                data['close'] = pd.to_numeric(data['stck_clpr'], errors='coerce')
            if 'stck_oprc' in data.columns:
                data['open'] = pd.to_numeric(data['stck_oprc'], errors='coerce')
            if 'stck_hgpr' in data.columns:
                data['high'] = pd.to_numeric(data['stck_hgpr'], errors='coerce')
            if 'stck_lwpr' in data.columns:
                data['low'] = pd.to_numeric(data['stck_lwpr'], errors='coerce')
            if 'acml_vol' in data.columns:
                data['volume'] = pd.to_numeric(data['acml_vol'], errors='coerce')
                
            return data.sort_values('date').reset_index(drop=True)
            
        except Exception as e:
            self.logger.debug(f"ì¼ë´‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {stock_code}: {e}")
            return None
    
    def _analyze_enhanced_features(self, patterns: List[EnhancedWinLossPattern]) -> Dict[str, Any]:
        """í–¥ìƒëœ íŠ¹ì„± ë¶„ì„"""
        try:
            if not patterns:
                return {}
            
            # ìŠ¹ë¦¬/íŒ¨ë°° ê·¸ë£¹ ë¶„ë¦¬
            win_patterns = [p for p in patterns if p.is_win]
            loss_patterns = [p for p in patterns if not p.is_win]
            
            self.logger.info(f"ìŠ¹ë¦¬ íŒ¨í„´: {len(win_patterns)}ê°œ, íŒ¨ë°° íŒ¨í„´: {len(loss_patterns)}ê°œ")
            
            # ëª¨ë“  íŠ¹ì„± ìˆ˜ì§‘
            all_features = set()
            for pattern in patterns:
                all_features.update(pattern.enhanced_features.keys())
            
            # íŠ¹ì„±ë³„ ë¶„ì„
            feature_analysis = {}
            for feature in all_features:
                win_values = [p.enhanced_features.get(feature, 0) for p in win_patterns]
                loss_values = [p.enhanced_features.get(feature, 0) for p in loss_patterns]
                
                if not win_values or not loss_values:
                    continue
                
                win_mean = np.mean(win_values)
                loss_mean = np.mean(loss_values)
                win_std = np.std(win_values)
                loss_std = np.std(loss_values)
                
                # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
                t_stat, p_value = self._enhanced_t_test(win_values, loss_values)
                
                # íŠ¹ì„± ê°€ì¤‘ì¹˜ ê³„ì‚°
                weight = abs(win_mean - loss_mean) / (win_std + loss_std + 1e-8)
                
                feature_analysis[feature] = {
                    'win_mean': win_mean,
                    'loss_mean': loss_mean,
                    'win_std': win_std,
                    'loss_std': loss_std,
                    'difference': win_mean - loss_mean,
                    'weight': weight,
                    't_stat': t_stat,
                    'p_value': p_value,
                    'significance': p_value < 0.05,
                    'effect_size': abs(win_mean - loss_mean) / np.sqrt((win_std**2 + loss_std**2) / 2)
                }
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            max_weight = max([fa['weight'] for fa in feature_analysis.values()]) if feature_analysis else 1
            for feature in feature_analysis:
                feature_analysis[feature]['normalized_weight'] = feature_analysis[feature]['weight'] / max_weight
            
            # ìƒìœ„ íŠ¹ì„± ì„ íƒ
            top_features = self._get_top_enhanced_features(feature_analysis)
            
            return {
                'feature_analysis': feature_analysis,
                'top_features': top_features,
                'total_features': len(feature_analysis),
                'significant_features': len([f for f in feature_analysis.values() if f['significance']])
            }
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒëœ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _enhanced_t_test(self, group1: List[float], group2: List[float]) -> Tuple[float, float]:
        """í–¥ìƒëœ t-test"""
        try:
            n1, n2 = len(group1), len(group2)
            if n1 < 2 or n2 < 2:
                return 0.0, 1.0
            
            mean1, mean2 = np.mean(group1), np.mean(group2)
            var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
            
            # Welch's t-test (ì´ë¶„ì‚° ê°€ì •)
            se = np.sqrt(var1/n1 + var2/n2)
            t_stat = (mean1 - mean2) / se
            
            # ììœ ë„ ê³„ì‚°
            df = (var1/n1 + var2/n2)**2 / ((var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1))
            
            # p-value ê³„ì‚° (ê°„ë‹¨í•œ ê·¼ì‚¬)
            p_value = 2 * (1 - self._t_cdf(abs(t_stat), df))
            
            return t_stat, p_value
            
        except Exception as e:
            self.logger.debug(f"t-test ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0, 1.0
    
    def _t_cdf(self, t: float, df: float) -> float:
        """t-ë¶„í¬ ëˆ„ì ë¶„í¬í•¨ìˆ˜ ê·¼ì‚¬"""
        # ê°„ë‹¨í•œ ê·¼ì‚¬ (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ êµ¬í˜„ í•„ìš”)
        if df > 30:
            return 0.5 + 0.5 * np.tanh(t / 2)
        else:
            return 0.5 + 0.5 * np.tanh(t / (1 + df/10))
    
    def _get_top_enhanced_features(self, feature_analysis: Dict[str, Dict]) -> List[Dict]:
        """ìƒìœ„ í–¥ìƒëœ íŠ¹ì„± ì„ íƒ"""
        try:
            # ê°€ì¤‘ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_features = sorted(
                feature_analysis.items(),
                key=lambda x: x[1]['normalized_weight'],
                reverse=True
            )
            
            top_features = []
            for feature, analysis in sorted_features[:20]:  # ìƒìœ„ 20ê°œ
                top_features.append({
                    'feature': feature,
                    'analysis': analysis,
                    'rank': len(top_features) + 1
                })
            
            return top_features
            
        except Exception as e:
            self.logger.error(f"ìƒìœ„ íŠ¹ì„± ì„ íƒ ì‹¤íŒ¨: {e}")
            return []
    
    def _save_analysis_result(self, result: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            # JSON ì €ì¥
            with open('enhanced_analysis_with_auto_collection.json', 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info("âœ… í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger = setup_logger(__name__)
    
    # ìë™ ë°ì´í„° ìˆ˜ì§‘ í†µí•© ë¶„ì„ê¸° ì‹¤í–‰
    analyzer = EnhancedAnalyzerWithAutoCollection(logger)
    
    # ë¶„ì„ ì‹¤í–‰
    results = analyzer.analyze_with_auto_collection()
    
    if results:
        print("\n" + "="*80)
        print("ğŸ“Š ìë™ ë°ì´í„° ìˆ˜ì§‘ í†µí•© í–¥ìƒëœ íŒ¨í„´ ë¶„ì„ ê²°ê³¼")
        print("="*80)
        print(f"ì´ íŒ¨í„´ ìˆ˜: {results['total_patterns']}")
        print(f"ìŠ¹ë¦¬ íŒ¨í„´: {results['win_patterns']}")
        print(f"íŒ¨ë°° íŒ¨í„´: {results['loss_patterns']}")
        print(f"ì „ì²´ ìŠ¹ë¥ : {results['win_rate']:.1%}")
        print(f"ì²˜ë¦¬ëœ ì¢…ëª©: {results['processed_stocks']}ê°œ")
        print(f"ê±´ë„ˆëœ€: {results['skipped_logs']}ê±´")
        print(f"ì´ íŠ¹ì„± ìˆ˜: {results['total_features']}")
        print(f"ìœ ì˜í•œ íŠ¹ì„± ìˆ˜: {results['significant_features']}")
        
        # ìƒìœ„ íŠ¹ì„± ì¶œë ¥
        print("\nğŸ” ìƒìœ„ íŠ¹ì„±:")
        for i, feature_info in enumerate(results['top_features'][:15], 1):
            feature = feature_info['feature']
            analysis = feature_info['analysis']
            print(f"{i:2d}. {feature}")
            print(f"    ìŠ¹ë¦¬ í‰ê· : {analysis['win_mean']:.3f}")
            print(f"    íŒ¨ë°° í‰ê· : {analysis['loss_mean']:.3f}")
            print(f"    ì°¨ì´: {analysis['difference']:+.3f}")
            print(f"    ê°€ì¤‘ì¹˜: {analysis['normalized_weight']:.3f}")
            print(f"    ìœ ì˜ì„±: {'âœ…' if analysis['significance'] else 'âŒ'}")
            print(f"    íš¨ê³¼í¬ê¸°: {analysis['effect_size']:.3f}")
            print()
        
        # ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼
        if 'collection_results' in results:
            collection_results = results['collection_results']
            successful_collections = sum(collection_results.values())
            total_collections = len(collection_results)
            print(f"ğŸ“ˆ ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼: {successful_collections}/{total_collections}ê°œ ì„±ê³µ")
        
        print("\nâœ… ìë™ ë°ì´í„° ìˆ˜ì§‘ í†µí•© ë¶„ì„ ì™„ë£Œ!")
        
    else:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
