"""
ì¼ë´‰ ê¸°ë°˜ ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë¶„ì„ê¸°
ë¶„ë´‰ ë°ì´í„°ì™€ ì¼ë´‰ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ìŠ¹ë¦¬ í™•ë¥ ì„ ë†’ì´ëŠ” íŒ¨í„´ì„ ë¶„ì„
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
from pathlib import Path
import pickle
import json
import re

from utils.logger import setup_logger
from utils.korean_time import now_kst


@dataclass
class PatternFeature:
    """íŒ¨í„´ íŠ¹ì„± ë°ì´í„° í´ë˜ìŠ¤"""
    feature_name: str
    value: float
    weight: float = 1.0
    description: str = ""


@dataclass
class WinLossPattern:
    """ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë°ì´í„° í´ë˜ìŠ¤"""
    stock_code: str
    signal_date: str
    signal_time: str
    is_win: bool
    daily_features: Dict[str, float]
    minute_features: Dict[str, float]
    combined_score: float = 0.0


class DailyPatternAnalyzer:
    """ì¼ë´‰ ê¸°ë°˜ íŒ¨í„´ ë¶„ì„ê¸°"""
    
    def __init__(self, logger=None):
        self.logger = logger or setup_logger(__name__)
        self.patterns: List[WinLossPattern] = []
        self.feature_weights: Dict[str, float] = {}
        self.win_threshold: float = 0.6  # ìŠ¹ë¦¬ í™•ë¥  ì„ê³„ê°’
        
    def load_signal_replay_logs(self, log_dir: str = "signal_replay_log") -> List[Dict]:
        """ì‹œë®¬ë ˆì´ì…˜ ë¡œê·¸ì—ì„œ ìŠ¹ë¦¬/íŒ¨ë°° ë°ì´í„° ë¡œë“œ"""
        try:
            log_path = Path(log_dir)
            if not log_path.exists():
                self.logger.warning(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {log_dir}")
                return []
            
            logs = []
            for log_file in log_path.glob("*.txt"):
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        logs.extend(self._parse_log_content(content, log_file.name))
                except Exception as e:
                    self.logger.warning(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨ {log_file}: {e}")
            
            self.logger.info(f"ì´ {len(logs)}ê°œì˜ ê±°ë˜ ë¡œê·¸ ë¡œë“œ ì™„ë£Œ")
            return logs
            
        except Exception as e:
            self.logger.error(f"ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_log_content(self, content: str, filename: str) -> List[Dict]:
        """ë¡œê·¸ ë‚´ìš© íŒŒì‹±"""
        logs = []
        lines = content.split('\n')
        
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        date_match = re.search(r'(\d{8})', filename)
        signal_date = date_match.group(1) if date_match else now_kst().strftime("%Y%m%d")
        
        current_stock = None
        current_trades = []
        
        for line in lines:
            line = line.strip()
            
            # ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
            stock_match = re.search(r'=== (\d{6}) -', line)
            if stock_match:
                current_stock = stock_match.group(1)
                current_trades = []
                continue
            
            # ì²´ê²° ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ê±°ë˜ ì •ë³´ ì¶”ì¶œ
            if "ì²´ê²° ì‹œë®¬ë ˆì´ì…˜:" in line:
                continue
            
            # ë§¤ìˆ˜/ë§¤ë„ ì •ë³´ ì¶”ì¶œ
            trade_match = re.search(r'(\d{2}:\d{2}) ë§¤ìˆ˜\[.*?\] @([\d,]+) â†’ (\d{2}:\d{2}) ë§¤ë„\[.*?\] @([\d,]+) \(([+-]?\d+\.\d+)%\)', line)
            if trade_match and current_stock:
                buy_time = trade_match.group(1)
                buy_price = float(trade_match.group(2).replace(',', ''))
                sell_time = trade_match.group(3)
                sell_price = float(trade_match.group(4).replace(',', ''))
                return_pct = float(trade_match.group(5))
                
                is_win = return_pct > 0
                
                log_data = {
                    'stock_code': current_stock,
                    'signal_date': signal_date,
                    'signal_time': buy_time,
                    'is_win': is_win,
                    'return_pct': return_pct,
                    'buy_price': buy_price,
                    'sell_price': sell_price,
                    'raw_line': line
                }
                
                logs.append(log_data)
        
        return logs
    
    def _extract_trade_info(self, line: str, is_win: bool) -> Optional[Dict]:
        """ê±°ë˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: 09:03, 10:15 ë“±)
            import re
            time_match = re.search(r'(\d{2}:\d{2})', line)
            if not time_match:
                return None
            
            signal_time = time_match.group(1)
            
            # ì¢…ëª©ì½”ë“œ ì¶”ì¶œ (ì˜ˆ: 103840)
            stock_match = re.search(r'(\d{6})', line)
            if not stock_match:
                return None
            
            stock_code = stock_match.group(1)
            
            # ë‚ ì§œëŠ” íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ (ì˜ˆ: signal_replay_20250901.txt)
            date_match = re.search(r'(\d{8})', line)
            signal_date = date_match.group(1) if date_match else now_kst().strftime("%Y%m%d")
            
            return {
                'stock_code': stock_code,
                'signal_date': signal_date,
                'signal_time': signal_time,
                'is_win': is_win,
                'raw_line': line
            }
            
        except Exception as e:
            self.logger.debug(f"ê±°ë˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_daily_data(self, stock_code: str, date_str: str) -> Optional[pd.DataFrame]:
        """ì¼ë´‰ ë°ì´í„° ë¡œë“œ"""
        try:
            daily_cache_dir = Path("cache/daily_data")
            daily_file = daily_cache_dir / f"{stock_code}_daily.pkl"
            
            if not daily_file.exists():
                return None
                
            with open(daily_file, 'rb') as f:
                data = pickle.load(f)
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬ ë° ë°ì´í„° íƒ€ì… ë³€í™˜
            if 'stck_bsop_date' in data.columns:
                data['date'] = pd.to_datetime(data['stck_bsop_date'])
            if 'stck_clpr' in data.columns:
                data['close'] = pd.to_numeric(data['stck_clpr'], errors='coerce')
            if 'stck_oprc' in data.columns:
                data['open'] = pd.to_numeric(data['stck_oprc'], errors='coerce')
            if 'stck_hgpr' in data.columns:
                data['high'] = pd.to_numeric(data['stck_hgpr'], errors='coerce')
            if 'stck_lwpr' in data.columns:
                data['low'] = pd.to_numeric(data['stck_lwpr'], errors='coerce')
            if 'acml_vol' in data.columns:
                data['volume'] = pd.to_numeric(data['acml_vol'], errors='coerce')
                
            return data.sort_values('date').reset_index(drop=True)
            
        except Exception as e:
            self.logger.debug(f"ì¼ë´‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {stock_code}: {e}")
            return None
    
    def extract_daily_features(self, daily_data: pd.DataFrame, signal_date: str) -> Dict[str, float]:
        """ì¼ë´‰ ë°ì´í„°ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        
        try:
            if daily_data is None or daily_data.empty:
                return features
            
            # ì‹ í˜¸ ë‚ ì§œ ì´ì „ ë°ì´í„°ë§Œ ì‚¬ìš©
            signal_dt = pd.to_datetime(signal_date)
            historical_data = daily_data[daily_data['date'] < signal_dt].copy()
            
            if len(historical_data) < 5:
                return features
            
            # ìµœê·¼ 5ì¼, 10ì¼, 20ì¼ ë°ì´í„°
            recent_5d = historical_data.tail(5)
            recent_10d = historical_data.tail(10)
            recent_20d = historical_data.tail(20)
            
            # 1. ê°€ê²© ëª¨ë©˜í…€ íŠ¹ì„±
            features['price_momentum_5d'] = self._calculate_price_momentum(recent_5d)
            features['price_momentum_10d'] = self._calculate_price_momentum(recent_10d)
            features['price_momentum_20d'] = self._calculate_price_momentum(recent_20d)
            
            # 2. ê±°ë˜ëŸ‰ íŠ¹ì„±
            features['volume_ratio_5d'] = self._calculate_volume_ratio(recent_5d)
            features['volume_ratio_10d'] = self._calculate_volume_ratio(recent_10d)
            features['volume_ratio_20d'] = self._calculate_volume_ratio(recent_20d)
            
            # 3. ë³€ë™ì„± íŠ¹ì„±
            features['volatility_5d'] = self._calculate_volatility(recent_5d)
            features['volatility_10d'] = self._calculate_volatility(recent_10d)
            features['volatility_20d'] = self._calculate_volatility(recent_20d)
            
            # 4. ì¶”ì„¸ íŠ¹ì„±
            features['trend_strength_5d'] = self._calculate_trend_strength(recent_5d)
            features['trend_strength_10d'] = self._calculate_trend_strength(recent_10d)
            features['trend_strength_20d'] = self._calculate_trend_strength(recent_20d)
            
            # 5. ì§€ì§€/ì €í•­ íŠ¹ì„±
            features['support_resistance_ratio'] = self._calculate_support_resistance_ratio(historical_data)
            
            # 6. ì—°ì† ìƒìŠ¹/í•˜ë½ íŠ¹ì„±
            features['consecutive_up_days'] = self._calculate_consecutive_days(recent_10d, 'up')
            features['consecutive_down_days'] = self._calculate_consecutive_days(recent_10d, 'down')
            
            # 7. ê°­ íŠ¹ì„±
            features['gap_frequency'] = self._calculate_gap_frequency(recent_10d)
            features['gap_magnitude'] = self._calculate_gap_magnitude(recent_10d)
            
        except Exception as e:
            self.logger.debug(f"ì¼ë´‰ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return features
    
    def _calculate_price_momentum(self, data: pd.DataFrame) -> float:
        """ê°€ê²© ëª¨ë©˜í…€ ê³„ì‚°"""
        if len(data) < 2:
            return 0.0
        
        start_price = data['close'].iloc[0]
        end_price = data['close'].iloc[-1]
        
        if start_price == 0:
            return 0.0
        
        return (end_price - start_price) / start_price * 100
    
    def _calculate_volume_ratio(self, data: pd.DataFrame) -> float:
        """ê±°ë˜ëŸ‰ ë¹„ìœ¨ ê³„ì‚° (í‰ê·  ëŒ€ë¹„)"""
        if len(data) < 2:
            return 1.0
        
        recent_volume = data['volume'].iloc[-1]
        avg_volume = data['volume'].mean()
        
        if avg_volume == 0:
            return 1.0
        
        return recent_volume / avg_volume
    
    def _calculate_volatility(self, data: pd.DataFrame) -> float:
        """ë³€ë™ì„± ê³„ì‚° (ì¼ì¼ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨)"""
        if len(data) < 2:
            return 0.0
        
        returns = data['close'].pct_change().dropna()
        return returns.std() * 100
    
    def _calculate_trend_strength(self, data: pd.DataFrame) -> float:
        """ì¶”ì„¸ ê°•ë„ ê³„ì‚° (ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°)"""
        if len(data) < 3:
            return 0.0
        
        x = np.arange(len(data))
        y = data['close'].values
        
        # ì„ í˜• íšŒê·€
        coeffs = np.polyfit(x, y, 1)
        slope = coeffs[0]
        
        # ì •ê·œí™” (ê°€ê²© ëŒ€ë¹„)
        avg_price = data['close'].mean()
        if avg_price == 0:
            return 0.0
        
        return (slope / avg_price) * 100
    
    def _calculate_support_resistance_ratio(self, data: pd.DataFrame) -> float:
        """ì§€ì§€/ì €í•­ ë¹„ìœ¨ ê³„ì‚°"""
        if len(data) < 10:
            return 0.5
        
        recent_20d = data.tail(20)
        current_price = recent_20d['close'].iloc[-1]
        
        # ìµœê·¼ 20ì¼ ê³ ê°€/ì €ê°€ ë²”ìœ„ì—ì„œ í˜„ì¬ê°€ ìœ„ì¹˜
        high_20d = recent_20d['high'].max()
        low_20d = recent_20d['low'].min()
        
        if high_20d == low_20d:
            return 0.5
        
        return (current_price - low_20d) / (high_20d - low_20d)
    
    def _calculate_consecutive_days(self, data: pd.DataFrame, direction: str) -> int:
        """ì—°ì† ìƒìŠ¹/í•˜ë½ ì¼ìˆ˜ ê³„ì‚°"""
        if len(data) < 2:
            return 0
        
        consecutive = 0
        for i in range(len(data) - 1, 0, -1):
            if direction == 'up' and data['close'].iloc[i] > data['close'].iloc[i-1]:
                consecutive += 1
            elif direction == 'down' and data['close'].iloc[i] < data['close'].iloc[i-1]:
                consecutive += 1
            else:
                break
        
        return consecutive
    
    def _calculate_gap_frequency(self, data: pd.DataFrame) -> float:
        """ê°­ ë¹ˆë„ ê³„ì‚°"""
        if len(data) < 2:
            return 0.0
        
        gaps = 0
        for i in range(1, len(data)):
            prev_close = data['close'].iloc[i-1]
            curr_open = data['open'].iloc[i]
            
            # ê°­ í¬ê¸°ê°€ 1% ì´ìƒì¸ ê²½ìš°
            if abs(curr_open - prev_close) / prev_close > 0.01:
                gaps += 1
        
        return gaps / (len(data) - 1)
    
    def _calculate_gap_magnitude(self, data: pd.DataFrame) -> float:
        """ê°­ í¬ê¸° ê³„ì‚°"""
        if len(data) < 2:
            return 0.0
        
        gap_magnitudes = []
        for i in range(1, len(data)):
            prev_close = data['close'].iloc[i-1]
            curr_open = data['open'].iloc[i]
            
            if prev_close != 0:
                gap_mag = (curr_open - prev_close) / prev_close * 100
                gap_magnitudes.append(gap_mag)
        
        return np.mean(gap_magnitudes) if gap_magnitudes else 0.0
    
    def analyze_patterns(self, log_dir: str = "signal_replay_log") -> Dict[str, Any]:
        """ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë¶„ì„"""
        try:
            self.logger.info("ğŸ” ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë¶„ì„ ì‹œì‘...")
            
            # 1. ë¡œê·¸ ë°ì´í„° ë¡œë“œ
            logs = self.load_signal_replay_logs(log_dir)
            if not logs:
                self.logger.warning("ë¶„ì„í•  ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {}
            
            # 2. ê° ê±°ë˜ì— ëŒ€í•´ ì¼ë´‰ íŠ¹ì„± ì¶”ì¶œ
            patterns = []
            for log in logs:
                stock_code = log['stock_code']
                signal_date = log['signal_date']
                
                # ì¼ë´‰ ë°ì´í„° ë¡œë“œ
                daily_data = self.load_daily_data(stock_code, signal_date)
                if daily_data is None:
                    continue
                
                # ì¼ë´‰ íŠ¹ì„± ì¶”ì¶œ
                daily_features = self.extract_daily_features(daily_data, signal_date)
                
                # íŒ¨í„´ ê°ì²´ ìƒì„±
                pattern = WinLossPattern(
                    stock_code=stock_code,
                    signal_date=signal_date,
                    signal_time=log['signal_time'],
                    is_win=log['is_win'],
                    daily_features=daily_features,
                    minute_features={},  # ë¶„ë´‰ íŠ¹ì„±ì€ ë‚˜ì¤‘ì— ì¶”ê°€
                    combined_score=0.0
                )
                
                patterns.append(pattern)
            
            self.patterns = patterns
            self.logger.info(f"âœ… {len(patterns)}ê°œ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
            
            # 3. íŠ¹ì„±ë³„ ìŠ¹ë¦¬/íŒ¨ë°° ì°¨ì´ ë¶„ì„
            analysis_result = self._analyze_feature_differences(patterns)
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_feature_differences(self, patterns: List[WinLossPattern]) -> Dict[str, Any]:
        """íŠ¹ì„±ë³„ ìŠ¹ë¦¬/íŒ¨ë°° ì°¨ì´ ë¶„ì„"""
        try:
            if not patterns:
                return {}
            
            # ìŠ¹ë¦¬/íŒ¨ë°° ê·¸ë£¹ ë¶„ë¦¬
            win_patterns = [p for p in patterns if p.is_win]
            loss_patterns = [p for p in patterns if not p.is_win]
            
            self.logger.info(f"ìŠ¹ë¦¬ íŒ¨í„´: {len(win_patterns)}ê°œ, íŒ¨ë°° íŒ¨í„´: {len(loss_patterns)}ê°œ")
            
            # ëª¨ë“  íŠ¹ì„± ìˆ˜ì§‘
            all_features = set()
            for pattern in patterns:
                all_features.update(pattern.daily_features.keys())
            
            # íŠ¹ì„±ë³„ ë¶„ì„
            feature_analysis = {}
            for feature in all_features:
                win_values = [p.daily_features.get(feature, 0) for p in win_patterns]
                loss_values = [p.daily_features.get(feature, 0) for p in loss_patterns]
                
                if not win_values or not loss_values:
                    continue
                
                win_mean = np.mean(win_values)
                loss_mean = np.mean(loss_values)
                win_std = np.std(win_values)
                loss_std = np.std(loss_values)
                
                # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ê°„ë‹¨í•œ t-test)
                t_stat, p_value = self._simple_t_test(win_values, loss_values)
                
                # íŠ¹ì„± ê°€ì¤‘ì¹˜ ê³„ì‚° (ìŠ¹ë¦¬ì™€ íŒ¨ë°°ì˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
                weight = abs(win_mean - loss_mean) / (win_std + loss_std + 1e-8)
                
                feature_analysis[feature] = {
                    'win_mean': win_mean,
                    'loss_mean': loss_mean,
                    'win_std': win_std,
                    'loss_std': loss_std,
                    'difference': win_mean - loss_mean,
                    'weight': weight,
                    't_stat': t_stat,
                    'p_value': p_value,
                    'significance': p_value < 0.05
                }
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            max_weight = max([fa['weight'] for fa in feature_analysis.values()])
            for feature in feature_analysis:
                feature_analysis[feature]['normalized_weight'] = feature_analysis[feature]['weight'] / max_weight
            
            # ê²°ê³¼ ì •ë¦¬
            result = {
                'total_patterns': len(patterns),
                'win_patterns': len(win_patterns),
                'loss_patterns': len(loss_patterns),
                'win_rate': len(win_patterns) / len(patterns) if patterns else 0,
                'feature_analysis': feature_analysis,
                'top_features': self._get_top_features(feature_analysis)
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ì°¨ì´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _simple_t_test(self, group1: List[float], group2: List[float]) -> Tuple[float, float]:
        """ê°„ë‹¨í•œ t-test êµ¬í˜„"""
        try:
            n1, n2 = len(group1), len(group2)
            if n1 < 2 or n2 < 2:
                return 0.0, 1.0
            
            mean1, mean2 = np.mean(group1), np.mean(group2)
            var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
            
            # í’€ë“œ ë¶„ì‚°
            pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)
            
            # í‘œì¤€ ì˜¤ì°¨
            se = np.sqrt(pooled_var * (1/n1 + 1/n2))
            
            if se == 0:
                return 0.0, 1.0
            
            # t-í†µê³„ëŸ‰
            t_stat = (mean1 - mean2) / se
            
            # ììœ ë„
            df = n1 + n2 - 2
            
            # p-value (ê·¼ì‚¬ì¹˜)
            p_value = 2 * (1 - self._t_cdf(abs(t_stat), df))
            
            return t_stat, p_value
            
        except Exception as e:
            self.logger.debug(f"t-test ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0, 1.0
    
    def _t_cdf(self, t: float, df: int) -> float:
        """t-ë¶„í¬ ëˆ„ì ë¶„í¬í•¨ìˆ˜ ê·¼ì‚¬"""
        # ê°„ë‹¨í•œ ê·¼ì‚¬ì¹˜ (ì •í™•í•œ ê³„ì‚°ì€ scipy.stats.t.cdf ì‚¬ìš© ê¶Œì¥)
        if df > 30:
            # ì •ê·œë¶„í¬ë¡œ ê·¼ì‚¬
            return 0.5 * (1 + np.tanh(t / np.sqrt(2)))
        else:
            # ììœ ë„ê°€ ì‘ì„ ë•Œì˜ ê·¼ì‚¬
            return 0.5 * (1 + np.tanh(t / np.sqrt(df / (df - 2))))
    
    def _get_top_features(self, feature_analysis: Dict[str, Any], top_n: int = 10) -> List[Dict[str, Any]]:
        """ìƒìœ„ íŠ¹ì„± ì¶”ì¶œ"""
        sorted_features = sorted(
            feature_analysis.items(),
            key=lambda x: x[1]['normalized_weight'],
            reverse=True
        )
        
        return [
            {
                'feature': feature,
                'analysis': analysis
            }
            for feature, analysis in sorted_features[:top_n]
        ]
    
    def save_analysis_results(self, results: Dict[str, Any], output_file: str = "daily_pattern_analysis.json"):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def generate_filter_rules(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """í•„í„° ê·œì¹™ ìƒì„±"""
        try:
            if not analysis_results or 'feature_analysis' not in analysis_results:
                return {}
            
            feature_analysis = analysis_results['feature_analysis']
            top_features = analysis_results.get('top_features', [])
            
            # ìƒìœ„ íŠ¹ì„±ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•„í„° ê·œì¹™ ìƒì„±
            filter_rules = {}
            
            for feature_info in top_features[:5]:  # ìƒìœ„ 5ê°œ íŠ¹ì„±ë§Œ ì‚¬ìš©
                feature = feature_info['feature']
                analysis = feature_info['analysis']
                
                if not analysis['significance']:
                    continue
                
                # ìŠ¹ë¦¬ íŒ¨í„´ì˜ í‰ê· ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ í•„í„° ìƒì„±
                win_mean = analysis['win_mean']
                win_std = analysis['win_std']
                
                # ì„ê³„ê°’ ì„¤ì • (í‰ê·  Â± 0.5 * í‘œì¤€í¸ì°¨)
                threshold_low = win_mean - 0.5 * win_std
                threshold_high = win_mean + 0.5 * win_std
                
                filter_rules[feature] = {
                    'threshold_low': threshold_low,
                    'threshold_high': threshold_high,
                    'weight': analysis['normalized_weight'],
                    'description': f"{feature}: {threshold_low:.3f} ~ {threshold_high:.3f}"
                }
            
            return filter_rules
            
        except Exception as e:
            self.logger.error(f"í•„í„° ê·œì¹™ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = DailyPatternAnalyzer()
    
    # íŒ¨í„´ ë¶„ì„ ì‹¤í–‰
    results = analyzer.analyze_patterns()
    
    if results:
        # ê²°ê³¼ ì €ì¥
        analyzer.save_analysis_results(results)
        
        # í•„í„° ê·œì¹™ ìƒì„±
        filter_rules = analyzer.generate_filter_rules(results)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š ì¼ë´‰ ê¸°ë°˜ ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ ë¶„ì„ ê²°ê³¼")
        print("="*80)
        print(f"ì´ íŒ¨í„´ ìˆ˜: {results['total_patterns']}")
        print(f"ìŠ¹ë¦¬ íŒ¨í„´: {results['win_patterns']}")
        print(f"íŒ¨ë°° íŒ¨í„´: {results['loss_patterns']}")
        print(f"ì „ì²´ ìŠ¹ë¥ : {results['win_rate']:.1%}")
        print("\nğŸ” ìƒìœ„ íŠ¹ì„±:")
        
        for i, feature_info in enumerate(results['top_features'][:10], 1):
            feature = feature_info['feature']
            analysis = feature_info['analysis']
            print(f"{i:2d}. {feature}")
            print(f"    ìŠ¹ë¦¬ í‰ê· : {analysis['win_mean']:.3f}")
            print(f"    íŒ¨ë°° í‰ê· : {analysis['loss_mean']:.3f}")
            print(f"    ì°¨ì´: {analysis['difference']:+.3f}")
            print(f"    ê°€ì¤‘ì¹˜: {analysis['normalized_weight']:.3f}")
            print(f"    ìœ ì˜ì„±: {'âœ…' if analysis['significance'] else 'âŒ'}")
            print()
        
        print("\nğŸ¯ ìƒì„±ëœ í•„í„° ê·œì¹™:")
        for feature, rule in filter_rules.items():
            print(f"â€¢ {rule['description']} (ê°€ì¤‘ì¹˜: {rule['weight']:.3f})")
    
    else:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
