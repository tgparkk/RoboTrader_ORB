#!/usr/bin/env python3
"""
ìë™ ì¼ì¹˜ì„± ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ë§¤ì¼ 15:40ì— ì‹¤í–‰í•˜ì—¬ ì‹¤ì‹œê°„ ë°ì´í„°ì™€ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì˜ ì¼ì¹˜ì„±ì„ ìë™ ê²€ì¦
"""
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import pickle
import pandas as pd
from datetime import datetime
from utils.logger import setup_logger

logger = setup_logger(__name__)


def verify_data_consistency(date_str: str):
    """
    ë°ì´í„° ì¼ì¹˜ì„± ê²€ì¦
    
    Args:
        date_str: ê²€ì¦í•  ë‚ ì§œ (YYYYMMDD)
    """
    cache_dir = Path("cache/minute_data")
    
    if not cache_dir.exists():
        logger.error(f"ìºì‹œ ë””ë ‰í† ë¦¬ ì—†ìŒ: {cache_dir}")
        return {'success': False, 'message': 'ìºì‹œ ë””ë ‰í† ë¦¬ ì—†ìŒ'}
    
    # í•´ë‹¹ ë‚ ì§œì˜ íŒŒì¼ ì°¾ê¸°
    files = list(cache_dir.glob(f"*_{date_str}.pkl"))
    
    if not files:
        logger.warning(f"í•´ë‹¹ ë‚ ì§œ íŒŒì¼ ì—†ìŒ: {date_str}")
        return {'success': False, 'message': f'íŒŒì¼ ì—†ìŒ: {date_str}'}
    
    logger.info(f"âœ… ê²€ì¦ ëŒ€ìƒ: {len(files)}ê°œ íŒŒì¼")
    
    # ê° íŒŒì¼ ê²€ì¦
    results = {
        'date': date_str,
        'total_files': len(files),
        'valid_files': 0,
        'invalid_files': 0,
        'issues': []
    }
    
    for file in files:
        stock_code = file.stem.split('_')[0]
        
        try:
            with open(file, 'rb') as f:
                data = pickle.load(f)
            
            if data is None or data.empty:
                results['invalid_files'] += 1
                results['issues'].append(f"{stock_code}: ë¹ˆ ë°ì´í„°")
                continue
            
            # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
            issues = []
            
            # 1. ìµœì†Œ ë°ì´í„° ê°œìˆ˜ (09:00~15:30 = 390ê°œ)
            if len(data) < 300:
                issues.append(f"ë°ì´í„° ë¶€ì¡± ({len(data)}/390)")
            
            # 2. datetime ì»¬ëŸ¼ ì¡´ì¬
            if 'datetime' not in data.columns and 'time' not in data.columns:
                issues.append("ì‹œê°„ ì»¬ëŸ¼ ì—†ìŒ")
            
            # 3. OHLCV ì»¬ëŸ¼ ì¡´ì¬
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            missing_cols = [col for col in required_cols if col not in data.columns]
            if missing_cols:
                issues.append(f"ëˆ„ë½ ì»¬ëŸ¼: {missing_cols}")
            
            # 4. 09:00 ì‹œì‘ í™•ì¸
            if 'datetime' in data.columns:
                data['datetime'] = pd.to_datetime(data['datetime'])
                first_time = data['datetime'].iloc[0]
                if first_time.hour != 9 or first_time.minute != 0:
                    issues.append(f"09:00 ì‹œì‘ ì•„ë‹˜ ({first_time.strftime('%H:%M')})")
            
            # 5. 15:00 ì´í›„ ë°ì´í„° í¬í•¨
            if 'datetime' in data.columns:
                has_afternoon = any(data['datetime'].dt.hour >= 15)
                if not has_afternoon:
                    issues.append("15:00 ì´í›„ ë°ì´í„° ì—†ìŒ")
            
            # 6. ì¤‘ë³µ ê²€ì‚¬
            if 'datetime' in data.columns:
                duplicates = data['datetime'].duplicated().sum()
                if duplicates > 0:
                    issues.append(f"ì¤‘ë³µ ë°ì´í„°: {duplicates}ê°œ")
            
            # 7. ì‹œê°„ ì—°ì†ì„± ê²€ì‚¬ (ìƒ˜í”Œ)
            if 'datetime' in data.columns and len(data) > 10:
                # ì²˜ìŒ 10ê°œ ë¶„ë´‰ ì—°ì†ì„± í™•ì¸
                for i in range(1, min(10, len(data))):
                    time_diff = (data['datetime'].iloc[i] - data['datetime'].iloc[i-1]).total_seconds()
                    if time_diff != 60:  # 1ë¶„ ê°„ê²© ì•„ë‹˜
                        issues.append(f"ì‹œê°„ ë¶ˆì—°ì†: {data['datetime'].iloc[i-1].strftime('%H:%M')} â†’ {data['datetime'].iloc[i].strftime('%H:%M')}")
                        break
            
            if issues:
                results['invalid_files'] += 1
                results['issues'].append(f"{stock_code}: {', '.join(issues)}")
                logger.warning(f"âš ï¸ {stock_code}: {', '.join(issues)}")
            else:
                results['valid_files'] += 1
                logger.debug(f"âœ… {stock_code}: ì •ìƒ ({len(data)}ê°œ)")
        
        except Exception as e:
            results['invalid_files'] += 1
            results['issues'].append(f"{stock_code}: ë¡œë“œ ì‹¤íŒ¨ ({e})")
            logger.error(f"âŒ {stock_code}: {e}")
    
    # ê²°ê³¼ ìš”ì•½
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“Š ê²€ì¦ ê²°ê³¼")
    logger.info(f"{'='*80}")
    logger.info(f"ë‚ ì§œ: {date_str}")
    logger.info(f"ì „ì²´ íŒŒì¼: {results['total_files']}ê°œ")
    logger.info(f"âœ… ì •ìƒ: {results['valid_files']}ê°œ ({results['valid_files']/results['total_files']*100:.1f}%)")
    logger.info(f"âš ï¸ ì´ìƒ: {results['invalid_files']}ê°œ ({results['invalid_files']/results['total_files']*100:.1f}%)")
    
    if results['issues']:
        logger.info(f"\nğŸ” ë°œê²¬ëœ ë¬¸ì œì :")
        for issue in results['issues'][:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            logger.info(f"   - {issue}")
        if len(results['issues']) > 10:
            logger.info(f"   ... ì™¸ {len(results['issues']) - 10}ê±´")
    
    results['success'] = results['invalid_files'] == 0
    
    return results


def generate_report(date_str: str, output_file: str = None):
    """ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    if output_file is None:
        output_file = f"verification_report_{date_str}.txt"
    
    results = verify_data_consistency(date_str)
    
    if not results['success']:
        logger.warning(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨: {results['message'] if 'message' in results else 'ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ'}")
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"{'='*100}\n")
            f.write(f"ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë¦¬í¬íŠ¸\n")
            f.write(f"{'='*100}\n\n")
            f.write(f"ë‚ ì§œ: {date_str}\n")
            f.write(f"ê²€ì¦ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"\n")
            f.write(f"{'='*100}\n")
            f.write(f"ê²€ì¦ ê²°ê³¼\n")
            f.write(f"{'='*100}\n\n")
            f.write(f"ì „ì²´ íŒŒì¼: {results['total_files']}ê°œ\n")
            f.write(f"ì •ìƒ: {results['valid_files']}ê°œ ({results['valid_files']/results['total_files']*100:.1f}%)\n")
            f.write(f"ì´ìƒ: {results['invalid_files']}ê°œ ({results['invalid_files']/results['total_files']*100:.1f}%)\n")
            f.write(f"\n")
            
            if results['issues']:
                f.write(f"{'='*100}\n")
                f.write(f"ë°œê²¬ëœ ë¬¸ì œì \n")
                f.write(f"{'='*100}\n\n")
                for issue in results['issues']:
                    f.write(f"- {issue}\n")
            
            f.write(f"\n")
            f.write(f"{'='*100}\n")
            f.write(f"ê²€ì¦ ì™„ë£Œ\n")
            f.write(f"{'='*100}\n")
        
        logger.info(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
        
    except Exception as e:
        logger.error(f"âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return results


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‹¤ì‹œê°„ ë°ì´í„° ìë™ ê²€ì¦")
    parser.add_argument('--date', type=str, help='ë‚ ì§œ (YYYYMMDD), ë¯¸ì§€ì • ì‹œ ì˜¤ëŠ˜')
    parser.add_argument('--report', action='store_true', help='ë¦¬í¬íŠ¸ íŒŒì¼ ìƒì„±')
    
    args = parser.parse_args()
    
    # ë‚ ì§œ ì„¤ì •
    if args.date:
        date_str = args.date
    else:
        from utils.korean_time import now_kst
        date_str = now_kst().strftime('%Y%m%d')
    
    logger.info(f"ğŸ” ìë™ ê²€ì¦ ì‹œì‘: {date_str}")
    
    if args.report:
        results = generate_report(date_str)
    else:
        results = verify_data_consistency(date_str)
    
    # ì¢…ë£Œ ì½”ë“œ
    if results['success']:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == '__main__':
    main()

